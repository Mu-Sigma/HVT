---
title: "HVT Model diagnostics and validation"
author: "Zubin Dowlaty, Shubhra Prakash, Sunuganty Achyut Raj, Praditi Shah, Somya Shambhawi,Vishwavani"
date: "Built on: `r format(Sys.Date(), '%m/%d/%Y')`"
fig.height: 4
fig.width: 15
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth : 2
vignette: >
  %\VignetteIndexEntry{HVT Model diagnostics and validation}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{css, echo=FALSE}
/* CSS for floating TOC on the left side */
#TOC {
    /* float: left; */
    position: fixed;
    margin-left: -22vw;
    width: 18vw;
    height: fit-content;
    max-height: 82vh;
    overflow-y: auto;
    padding-top: 20px;
    padding-bottom: 20px;
    background-color: #f9f9f9;
    border-right: 1px solid #ddd;
    margin-top: -10em;
}

.main-container {
  margin-left: 220px; /* Adjust this value to match the width of the TOC + some margin */
}
body{
max-width:1200px;
width: 50%;
}
.caption {
  text-align: center;
}

p {
text-align: justify;
}
li {
  padding-bottom: 5px;
}
.superbigimage img {
     max-width: 97% !important;
}
@media screen and (max-width: 1200px) {
img {
     width: 100%
}
}
```



# 1. Abstract

The HVT package is a collection of R functions to facilitate building
<a href="https://link.springer.com/chapter/10.1007/1-84628-118-0_7" target="_blank">topology preserving maps</a> for rich
multivariate data. Tending towards a big data preponderance, a large
number of rows. A collection of R functions for this typical workflow is
organized below :

1.  **Data Compression**: Vector quantization (VQ), HVQ (hierarchical
    vector quantization) using means or medians. This step compresses
    the rows (long data frame) using a compression objective

2.  **Data Projection**: Dimension projection of the compressed cells to
    1D,2D and Interactive surface plot with the Sammons Nonlinear Algorithm. This step creates
    topology preserving map(also called as
    <a href="https://en.wikipedia.org/wiki/Embedding" target="_blank"> embedding </a> coordinates
    into the desired output dimension.

3.  **Tessellation**: Create cells required for object visualization
    using the Voronoi Tessellation method, package includes heatmap
    plots for hierarchical Voronoi tessellations (HVT). This step
    enables data insights, visualization, and interaction with the
    topology preserving map. Useful for semi-supervised tasks

4.  **Scoring**: Scoring new data sets and recording their assignment
    using the map objects from the above steps, in a sequence of maps if
    required

# 2. Data Compression

This package can perform vector quantization using the following
algorithms -

-   Hierarchical Vector Quantization using $k-means$
-   Hierarchical Vector Quantization using $k-medoids$

## 2.1 Using k-means

1.  The k-means algorithm randomly selects *k* data points as initial
    means
2.  *k* clusters are formed by assigning each data point to its closest
    cluster mean using the Euclidean distance
3.  Virtual means for each cluster are calculated by using all
    datapoints contained in a cluster

The second and third steps are iterated until a predefined number of
iterations is reached or the clusters converge. The runtime for the
algorithm is O(n).

## 2.2 Using k-medoids

1.  The k-medoids algorithm randomly selects *k* data points as initial
    means out of the n data points as the medoids.
2.  *k* clusters are formed by assigning each data point to its closest
    medoid by using any common distance metric methods.
3.  Virtual means for each cluster are calculated by using all
    datapoints contained in a cluster

The second and third steps are iterated until a predefined number of
iterations is reached or the clusters converge. The runtime for the
algorithm is O(k \* (n-k)\^2).

## 2.3 Hierarchical Vector Quantization

The algorithm divides the dataset recursively into cells using $k-means$
or $k-medoids$ algorithm. The maximum number of subsets are decided by
setting $n-cells$ to, say five, in order to divide the dataset into
maximum of five subsets. These five subsets are further divided into
five subsets(or less), resulting in a total of twenty five (5\*5)
subsets. The recursion terminates when the cells either contain less
than three data point or a stop criterion is reached. In this case, the
stop criterion is set to when the cell error exceeds the quantization
threshold.

The steps for this method are as follows :

1.  Select k(number of cells), depth and quantization error threshold
2.  Perform quantization (using $k-means$ or $k-medoids$) on the input
    dataset
3.  Calculate quantization error for each of the k cells
4.  Compare the quantization error for each cell to quantization error
    threshold
5.  Repeat steps 2 to 4 for each of the k cells whose quantization error
    is above threshold until stop criterion is reached.

The stop criterion is when the quantization error of a cell satisfies
one of the below conditions

-   reaches below quantization error threshold
-   there are less than three data point in the cell
-   the user specified depth has been attained

### 2.3.1 Quantization Error

Let us try to understand quantization error with an example.

```{r Quantization Error,echo=FALSE,warning=FALSE,fig.show='hold',message=FALSE,fig.cap='Figure 1: The Voronoi tessellation for level 1 shown for the 5 cells with the points overlayed', fig.width=4, fig.height=4}
knitr::include_graphics('./pngs/quant_explainer.png')
```

An example of a 2 dimensional VQ is shown above.

In the above image, we can see 5 cells with each cell containing a
certain number of points. The centroid for each cell is shown in blue.
These centroids are also known as codewords since they represent all the
points in that cell. The set of all codewords is called a codebook.

Now we want to calculate quantization error for each cell. For the sake
of simplicity, let's consider only one cell having centroid `A` and `m`
data points $F_i$ for calculating quantization error.

For each point, we calculate the distance between the point and the
centroid.

$$ d = ||A - F_i||_{p} $$

In the above equation, p = 1 means `L1_Norm` distance whereas p = 2
means `L2_Norm` distance. In the package, the `L1_Norm` distance is
chosen by default. The user can pass either `L1_Norm`, `L2_Norm` or a
custom function to calculate the distance between two points in n
dimensions.

$$QE  = \max(||A-F_i||_{p})$$

where

-   $A$ is the centroid of the cell
-   $F_i$ represents a data point in the cell
-   $p$ is the $p$-norm metric. Here $p$ = 1 represents L1 Norm and $p$
    = 2 represents L2 Norm.

Now, we take the maximum calculated distance of all m points. This gives
us the furthest distance of a point in the cell from the centroid, which
we refer to as `Quantization Error`. If the Quantization Error is higher
than the given threshold, the centroid/codevector is not a good
representation for the points in the cell. Now we can perform further
Vector Quantization on these points and repeat the above steps.

Please note that the user can select mean or max to calculate the
Quantization Error. The custom function takes a vector of m value (where
each value is a distance between point in `n` dimensions and centroids)
and returns a single value which is the Quantization Error for the cell.

If we select `mean` as the error metric, the above Quantization Error
equation will look like this :

$$QE  = \frac{1}{m}\sum_{i=1}^m||A-F_i||_{p}$$

where

-   $A$ is the centroid of the cell
-   $F_i$ represents a data point in the cell
-   $m$ is the number of points in the cell
-   $p$ is the $p$-norm metric. Here $p$ = 1 represents L1 Norm and $p$
    = 2 represents L2 Norm.

# 3. Data Projection

Sammon's projection is an algorithm that maps a high-dimensional space
to a space of lower dimensionality while attempting to preserve the
structure of inter-point distances in the projection. It is particularly
suited for use in exploratory data analysis and is usually considered a
non-linear approach since the mapping cannot be represented as a linear
combination of the original variables. The centroids are plotted in 2D
after performing Sammon's projection at every level of the tessellation.

Denoting the distance between $i^{th}$ and $j^{th}$ objects in the
original space by $d_{ij}^*$, and the distance between their projections
by $d_{ij}$. Sammon's mapping aims to minimize the below error function,
which is often referred to as Sammon's stress or Sammon's error

$$E=\frac{1}{\sum_{i<j} d_{ij}^*}\sum_{i<j}\frac{(d_{ij}^*-d_{ij})^2}{d_{ij}^*}$$

The minimization of this can be performed either by gradient descent, as
proposed initially, or by other means, usually involving iterative
methods. The number of iterations need to be experimentally determined
and convergent solutions are not always guaranteed. Many implementations
prefer to use the first Principal Components as a starting
configuration.

## 3.1 Tessellations

A Voronoi diagram is a way of dividing space into a number of regions. A
set of points (called seeds, sites, or generators) is specified
beforehand and for each seed, there will be a corresponding region
consisting of all points within proximity of that seed. These regions
are called Voronoi cells. It is complementary to Delaunay triangulation.

**Tessellate: Constructing Voronoi Tesselations**

In this package, we use `sammons` from the package `MASS` to project
higher dimensional data to a 2D space. The function `hvq` called from
the `trainHVT` function returns hierarchical quantized data which will be the
input for construction of the tessellations. The data is then
represented in 2D coordinates and the tessellations are plotted using
these coordinates as centroids. We use the package `deldir` for this
purpose. The `deldir` package computes the Delaunay triangulation (and
hence the Dirichlet or Voronoi tessellation) of a planar point set
according to the second (iterative) algorithm of Lee and Schacter. For
subsequent levels, transformation is performed on the 2D coordinates to
get all the points within its parent tile. Tessellations are plotted
using these transformed points as centroids. The lines in the
tessellations are chopped in places so that they do not protrude outside
the parent polygon. This is done for all the subsequent levels.

# 4. Notebook Requirements

This chunk verifies the installation of all the necessary packages to successfully run this vignette, if not, installs them and attach all the packages in the session environment.

```{r, message=FALSE, warning=FALSE}
list.of.packages <- c("dplyr","HVT", "kableExtra", "geozoo", "plotly", "purrr", "DT")

new.packages <-list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
if (length(new.packages))
  install.packages(new.packages, dependencies = TRUE, verbose = FALSE, repos='https://cloud.r-project.org/')

# Loading the required libraries
invisible(lapply(list.of.packages, library, character.only = TRUE))

```




# 5. Data Importing

## 5.1 Import Dataset from Local

The user can provide an absolute or relative path in the cell below to
access the data from his/her computer. User can set
`import_data_from_local` variable to TRUE to upload dataset from local.
<br> **Note: For this notebook import_data_from_local has been set to
FALSE as we are simulating a dataset in next section.**

```{r loading csv, message=FALSE, warning=FALSE}
import_data_from_local = FALSE # expects logical input

  file_name <- " " #enter the name of the local file
  file_path <- " " #enter the path of the local file

if(import_data_from_local){
  file_load <- paste0(file_path, file_name)
  dataset_updated <- as.data.frame(fread(file_load))
  if(nrow(dataset_updated) > 0){
    paste0("File ", file_name, " having ", nrow(dataset_updated), " row(s) and ", ncol(dataset_updated), " column(s)",  " imported successfully. ") %>% cat("\n")
    dataset_updated <- dataset_updated %>% mutate_if(is.numeric, round, digits = 4)
    paste0("Code chunk executed successfully. Below table showing first 10 row(s) of the dataset.") %>% cat("\n")
    dataset_updated %>% head(10) %>%as.data.frame() %>%DT::datatable(options = options, rownames = TRUE)
  }
  
} 
```

## 5.2 Simulate Dataset

In this section, we will use a simulated dataset. If you are not using
this option set `simulate_dataset` to FALSE. Given below is a simulated
dataset called torus that contains 12000 observations and 3 features. 

Let us see how to generate data for torus. We are using a library `geozoo` for this purpose. 
Geo Zoo (stands for Geometric Zoo) is a compilation of geometric objects ranging from 
3 to 10 dimensions. Geo Zoo contains regular or well-known objects, eg cube and sphere, and some abstract objects, e.g. Boy's surface, Torus and Hyper-Torus.

Here, we load the data and store into a variable `dataset_updated`.

```{r}
simulate_dataset= TRUE

if(simulate_dataset == TRUE){
  
set.seed(257)
##torus data generation
torus <- geozoo::torus(p = 3,n = 12000)
dataset_updated <- data.frame(torus$points)
colnames(dataset_updated) <- c("x","y","z")


  if(nrow(dataset_updated) > 0){
    paste0( "Dataset having ", nrow(dataset_updated), " row(s) and ", ncol(dataset_updated), " column(s)",  "simulated successfully. ") %>% cat("\n")  
    dataset_updated <- dataset_updated %>% mutate_if(is.numeric, round, digits = 4) 
    paste0("Code chunk executed successfully. The table below is showing first 10 row(s) of the dataset.") %>% cat("\n")
    dataset_updated %>% head(10) %>%as.data.frame() %>%displayTable()
  }
}
```



# 6. Data Understanding

## 6.1 Quick Peek of the Data

**Structure of Data**

In the below section we can see the structure of the data.

```{r struc_test3,  warning=FALSE, message=FALSE, error=FALSE}
dataset_updated %>% str()
```

## 6.2 Deleting Irrelevant Columns

The cell below will allow user to drop irrelevant column.

```{r Deleting irrelevant columns, warning=FALSE, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# Add column names which you want to remove
want_to_delete_column <- "no"

    del_col<-c(" `column_name` ")  

if(want_to_delete_column == "yes"){
   dataset_updated <-  dataset_updated[ , !(names(dataset_updated) %in% del_col)]
  print("Code chunk executed successfully. Overview of data types after removed selected columns")
  str( dataset_updated)
}else{
  paste0("No Columns removed. Please enter column name if you want to remove that column") %>% cat("\n")
}


```

## 6.3 Formatting and Renaming Columns

The code below contains a user defined function to rename or reformat
any column that the user chooses.

```{r Formatting and renaming columns, warning=FALSE, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# convert the column names to lower case
colnames( dataset_updated) <- colnames( dataset_updated) %>% casefold()

## rename column ?
want_to_rename_column <- "no" ## type "yes" if you want to rename a column

## renaming a column of a dataset 
rename_col_name <- " 'column_name` " ## use small letters
rename_col_name_to <- " `new_name` "

if(want_to_rename_column == "yes"){
  names( dataset_updated)[names( dataset_updated) == rename_col_name] <- rename_col_name_to
}

# remove space, comma, dot from column names
spaceless <- function(x) {colnames(x) <- gsub(pattern = "[^[:alnum:]]+",
                             replacement = ".",
                             names(x));x}
 dataset_updated <- spaceless( dataset_updated)

## below is the dataset summary
paste0("Successfully converted the column names to lower case and check the renamed column name if you changed") %>% cat("\n")
str( dataset_updated) ## showing summary for updated 

```

## 6.4 Changing Data Type of Columns

The section allows the user to change the data type of columns of his/her
choice.

```{r column type, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# If you want to change column type, change a below variable value to "yes"
want_to_change_column_type <- "no"

# you can change column type into numeric or character only
change_column_to_type <- "character" ## numeric

if(want_to_change_column_type == "yes" && change_column_to_type == "character"){
########################################################################################
################################## User Input Needed ###################################
########################################################################################
  select_columns <- c("panel_var") ###### Add column names you want to change here #####
   dataset_updated[select_columns]<- sapply( dataset_updated[select_columns],as.character)
  paste0("Code chunk executed successfully. Datatype of selected column(s) have been changed into numerical.")
  #str( dataset_updated)
}else if(want_to_change_column_type == "yes" && change_column_to_type == "numeric"){
  select_columns <- c('gearbox_oil_temperature')
   dataset_updated[select_columns]<- sapply( dataset_updated[select_columns],as.numeric)
  paste0("Code chunk executed successfully. Datatype of selected column(s) have been changed into categorical.")
  #str( dataset_updated)
}else{
  paste0("Datatype of columns have not been changed.") %>% cat("\n")
}
dataset_updated <- do.call(data.frame, dataset_updated)
str( dataset_updated)

```

## 6.5 Checking and Removing Duplicates

Presence of duplicate observations can be misleading, this sections
helps get rid of such rows in the dataset.

```{r,  message=FALSE, warning=FALSE, purl = FALSE}

want_to_remove_duplicates <- "yes"  ## type "no" for choosing to not remove duplicates

## removing duplicate observation if present in the dataset
if(want_to_remove_duplicates == "yes"){
  
   dataset_updated <-  dataset_updated %>% unique()
  paste0("Code chunk executed successfully, duplicates if present successfully removed. Updated dataset has ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s)") %>% print()
  cat("\n")
  str( dataset_updated) ## showing summary for updated dataset
} else{
  paste0("Code chunk executed successfully, NO duplicates were removed") %>% print()
}
```

## 6.6 List of Numerical and Categorical Column Names

```{r, warning=FALSE, message=FALSE, purl = FALSE}

# Return the column type 
CheckColumnType <- function(dataVector) {
  #Check if the column type is "numeric" or "character" & decide type accordingly
  if (class(dataVector) == "integer" || class(dataVector) == "numeric") {
    columnType <- "numeric"
  } else { columnType <- "character" }
  #Return the result
  return(columnType)
}
### Loading the list of numeric columns in variable
numeric_cols <<- colnames( dataset_updated)[unlist(sapply( dataset_updated, 
                                                       FUN = function(x){ CheckColumnType(x) == "numeric"}))]

### Loading the list of categorical columns in variable
cat_cols <- colnames( dataset_updated)[unlist(sapply( dataset_updated, 
                                                   FUN = function(x){ 
                                                     CheckColumnType(x) == "character"|| CheckColumnType(x) == "factor"}))]

### Removing Date Column from the list of categorical column
paste0("Code chunk executed successfully, list of numeric and categorical variables created.") %>% cat()
paste0("Numerical Column(s): \n Count : ", length(numeric_cols), "\n") %>% cat()
paste0(numeric_cols) %>% print()
paste0("Categorical Column(s): \n Count : ", length(cat_cols), "\n") %>% cat()
paste0(cat_cols) %>% print()
```

## 6.7 Filtering Dataset for Analysis

In this section, the dataset can be filtered for required row(s) for
further analysis.

```{r,  message=FALSE, warning=FALSE, purl = FALSE}

want_to_filter_dataset <- "no" ## type "yes" in case you want to filter
filter_col <- " "  ## Enter Column name to filter
filter_val <- " "  ## Enter Value to exclude for the column selected

if(want_to_filter_dataset == "yes"){
   dataset_updated <- filter_at( dataset_updated
                              , vars(contains(filter_col))
                              , all_vars(. != filter_val))
  
  paste0("Code chunk executed successfully, dataset filtered successfully on required columns. Updated dataset has ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s)") %>% print()
  cat("\n")
  str( dataset_updated) ## showing summary for updated dataset
  
} else{
  paste0("Code chunk executed successfully, entire dataset is available for analysis.") %>% print()
}
```



## 6.8 Missing Value Analysis

Missing values in the training data can lead to a biased model because
we have not analyzed the behavior and relationship of those values with
other variables correctly. It can lead to a wrong calculation or
classification. Missing values can be of 3 types:

-   **Missing Completely At Random (MCAR)**: When missing data are MCAR,
    the presence/absence of data is completely independent of observable
    variables and parameters of interest. This is a case when the
    probability of missing variables is the same for all observations.
    For example, respondents of the data collection process decide that
    they will declare they're earning after tossing a fair coin. If a
    head occurs, the respondent declares his / her earnings & vice
    versa.
-   **Missing At Random (MAR)**: When missing data is not random but can
    be related to an observed variable where there is complete
    information. This kind of missing data can induce a bias in the
    analysis, especially if it unbalances the data because of many
    missing values in a certain category. For example, we are collecting
    data for age and female has higher missing value compare to male.
-   **Missing Depending on Unobserved Predictors**: This is a case when
    the missing values are not random and are related to the unobserved
    input variable.

**Missing Value on Entire dataset**

```{r miss_value_entire_data, warning=FALSE, purl = FALSE}
na_total <- sum(is.na( dataset_updated))/prod(dim( dataset_updated))
if(na_total == 0){
  paste0("In the uploaded dataset, there is no missing value") %>% cat("\n")
}else{
  na_percentage <- paste0(sprintf(na_total*100, fmt = '%#.2f'),"%")
  paste0("Percentage of missing value in entire dataset is ",na_percentage) %>% cat("\n")
}

```

**Missing Value on Column-level**

The following code is to visualize the missing values (if any) using bar
chart.

`gg_miss_upset` function are using to visualize the patterns of
missingness, or rather the combinations of missingness across cases.

This function gives us(if any missing value present):

-   how many variables have missing values
-   which variable has the most missing values
-   And give us the variables which have missing values together

```{r miss_value_column_level, warning=FALSE,,fig.width=10, purl = FALSE}

# Below code gives you missing value in each column
paste0("Number of missing value in each column") %>% cat("\n")
print(sapply( dataset_updated, function(x) sum(is.na(x))))

missing_col_names <- names(which(sapply( dataset_updated, anyNA)))

total_na <- sum(is.na( dataset_updated))
# visualize the missing values (if any) using bar chart
if(total_na > 0 && length(missing_col_names) > 1){
  paste0("Code chunk executed successfully. Visualizing the missing values using bar chart") %>% cat("\n")
  gg_miss_upset( dataset_updated,
  nsets = 10,
  nintersects = NA)
}else if(total_na > 0){
   dataset_updated %>%
  DataExplorer::plot_missing() 
}else{
  paste("Code chunk executed successfully. No missing value exist.") %>% cat("\n")
}

```

**Missing Value Treatment**

In this section user can make decisions, how to tackle missing values in
dataset. Both column(s) and row(s) can be removed in the following
dataset based on the user choose to do so.


**Drop Column(s) with Missing Values**

The below code accepts user input and deletes the specified column.

```{r Col_Missing_Value_treatment,, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# OR do you want to drop column specific column
drop_cloumn_name_na <- "yes" ## type "yes" to drop column(s)
# write column name that you want to drop
drop_column_name <- c(" ") #enter column name
if(drop_cloumn_name_na == "yes"){
  names_df=names( dataset_updated) %in% drop_column_name
  dataset_updated <-  dataset_updated[ , which(!names( dataset_updated) %in% drop_column_name)]
  paste0("Code chunk executed, selected column(s) dropped successfully.") %>% print()
  cat("\n")
  str( dataset_updated)
} else {
  paste0("Code chunk executed, missing value not removed (if any).") %>% cat("\n")
  cat("\n")
}
```

**Drop Row(s) with Missing Values**

The below code accepts user input and deletes rows.

```{r Row_Missing_Value_treatment, message=FALSE, warning=FALSE, , purl = FALSE}

# Do you want to drop row(s) containing "NA"
drop_row <- "no" ## type "yes" to delete missing value observations
if(drop_row == "yes"){
   dataset_updated <-  dataset_updated %>% na.omit()
   paste0("Code chunk executed, missing values successfully identified and removed. Updated dataset has ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s)") %>% print()
  cat("\n")
} else{
  paste0("Code chunk executed, missing value(s) not removed (if any).") %>% cat("\n")
  cat("\n")
}
```

### 6.8.1 One-Hot Encoding

This technique bins all categorical values as either 1 or 0. It is used
for categorical variables with 2 classes. This is done because
classification models can only handle features that have numeric values.

Given below is the length of unique values in each categorical column

```{r}
cat_cols <-
  colnames(dataset_updated)[unlist(sapply(
    dataset_updated,
    FUN = function(x) {
      CheckColumnType(x) == "character" ||
        CheckColumnType(x) == "factor"
    }
  ))]

apply(dataset_updated[cat_cols], 2, function(x) {
  length(unique(x))
})
```

Selecting categorical columns with smaller unique values for
dummification

```{r dummify,purl = TRUE}
########################################################################################
################################## User Input Needed ###################################
########################################################################################
# Do you want to dummify the categorical variables?

dummify_cat <- FALSE ## TRUE,FALSE

# Select the columns on which dummification is to be performed
dum_cols <- c(" "," ") #enter column name in smalls

```

```{r dummify1, purl=FALSE,echo=dummify_cat}

# Below we dummify the columns mentioned by the user in the above code chunk
if(dummify_cat){
dummified_cols <- dataset_updated %>% dplyr::select(dum_cols) %>%       
  dummies::dummy.data.frame(dummy.classes = "ALL", sep = "_")  # dummy encode columns
names(dummified_cols) <- gsub(pattern = "[^[:alnum:]]+",
                             replacement = ".",
                             names(dummified_cols))

dataset_updated <- dataset_updated %>% 
  cbind(dummified_cols) %>%     # append encoded columns
  dplyr::select(-dum_cols)      # remove the old categorical columns
print("Shown below are the One-Hot Encoded columns")
print(dummified_cols %>% head(5) %>% DT::datatable(options = options) ) # Display columns
dummified_cols=names(dummified_cols)

# here we manipulate column names, round numeric values and display first 100 rows of data
colnames(dataset_updated) <- gsub(pattern = "[^[:alnum:]]+",
                             replacement = ".",
                             names(dataset_updated)) # Removes white-spaces between column names

dataset_updated <- dataset_updated %>% dplyr::mutate_if(is.numeric,round,digits = 3) # rounding digits in numeric values

dataset_updated %>% head(100) %>% DT::datatable(options = options)
} else{
 print("One-Hot Encoding was not performed on dataset.") 
}

```

### 6.8.2 Check for Singularity {#singularity}

```{r,purl = FALSE}
# Check data for singularity
singular_cols <- sapply(dataset_updated,function(x) length(unique(x))) %>%  # convert to dataframe
  data.frame(Unique_n = .) %>% dplyr::filter(Unique_n == 1) %>% 
  rownames() %>% data.frame(Constant_Variables = .)

if(nrow(singular_cols) != 0) {                              
  singular_cols  %>% DT::datatable()
} else {
  paste("There are no singular columns in the dataset") %>% htmltools::HTML()
}
# Display variance of columns
data <- dataset_updated %>% dplyr::summarise_if(is.numeric, var) %>% t() %>% 
  data.frame() %>% round(3) #%>% DT::datatable(colnames = "Variance")

colnames(data) <- c("Variance")
displayTable(data)

```

### 6.8.3 Selecting only Numeric Cols after Dummification

```{r}
numeric_cols=as.vector(sapply(dataset_updated, is.numeric))
dataset_updated=dataset_updated[,numeric_cols]
colnames(dataset_updated)
```



## 6.9 Final Dataset Summary

All further operations will be performed on the following dataset.

```{r Final_Dataset_Summary, ,warning=FALSE, purl = FALSE}
nums <- colnames(dataset_updated)[unlist(lapply(dataset_updated, is.numeric))]
cat(paste0("Final data frame contains ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s).","Code chunk executed. Below table showing first 10 row(s) of the dataset."))
dataset_updated <-  dataset_updated %>% mutate_if(is.numeric, round, digits = 4)

displayTable(dataset_updated[1:10,])

```


# 7. Data distribution {.tabset}

This section displays four objects.

**Variable Histograms**: The histogram distribution of all the features in the dataset.

**Box Plots**: Box plots for all the features in the dataset. These plots will display the median and Interquartile range of each column at a panel level.

**Correlation Matrix**: This calculates the Pearson correlation which is a bivariate correlation value measuring the linear correlation between two numeric columns. The output plot is shown as a matrix.

**Summary EDA**: The table provides descriptive statistics for all the features in the dataset.

- *variable*: The features/columns of the dataset
- *min*: Minimum value of that feature/column
- *1st Quartile*: The value that splits the lower 25% of the data when arranged in ascending order
- *median*: Middle value in the ascendingly ordered dataset
- *mean*: Sum of all values in the dataset divided by the total number of values
- *sd*: Measure of the dispersion of dataset relative to its mean.
- *3rd Quartile*: The value that splits the lower 75% of the data when arranged in ascending order
- *max*:  Maximum value of that feature/column
- *hist*: The basic barchart of the data distribution of a feature/column
- *n_row*: Number of rows for that feature/column
- *n_missing*: Number of missing values/NAs for that feature/column

It uses an inbuilt function called `edaPlots` to display the above-mentioned four objects.

## Summary Table
```{r}
edaPlots(dataset_updated, output_type = "summary", n_cols = 3)
```

## Histograms
```{r hist, figures-side, fig.show="hold", fig.height=2.2, fig.width =2.95, warning=FALSE, message=FALSE}
edaPlots(dataset_updated, output_type = "histogram", n_cols = 3)
```

## Box Plots
```{r  boxx, figures-side, fig.show="hold",fig.height=2.2, fig.width = 2.95,warning=FALSE, message=FALSE}
edaPlots(dataset_updated, output_type = "boxplot", n_cols = 3)
```

## Correlation Plot
```{r corr, warning=FALSE, message=FALSE, fig.align='center', fig.height = 4, fig.width = 4}
edaPlots(dataset_updated, output_type = "correlation", n_cols = 3)
```


## 7.1 Train - Test Split

Let us split the data into train and test. We will randomly select 80% of the
data as train and remaining as test.

```{r train-test dataset_updated,warning=FALSE,message=FALSE}
## 80% of the sample size
smp_size <- floor(0.80 * nrow(dataset_updated))

## set the seed to make your partition reproducible
set.seed(279)
train_ind <- sample(seq_len(nrow(dataset_updated)), size = smp_size)

dataset_updated_train <- dataset_updated[train_ind, ]
dataset_updated_test <- dataset_updated[-train_ind, ]
```

The train data contains `r nrow(dataset_updated_train)` rows and
`r ncol(dataset_updated_train)` columns. The test data contains
`r nrow(dataset_updated_test)` rows and `r ncol(dataset_updated_test)`
columns.

### 7.1.1 Train Distribution {.tabset}

#### Summary Table
```{r}
edaPlots(dataset_updated_train, output_type = "summary", n_cols = 3)
```

#### Histograms
```{r histtrain, figures-side, fig.show="hold", fig.height=2.2, fig.width =2.95, warning=FALSE, message=FALSE}
edaPlots(dataset_updated_train, output_type = "histogram", n_cols = 3)
```

#### Box Plots
```{r  boxxtrain, figures-side, fig.show="hold",fig.height=2.2, fig.width = 2.95,warning=FALSE, message=FALSE}
edaPlots(dataset_updated_train, output_type = "boxplot", n_cols = 3)
```

#### Correlation Plot
```{r corrtrain, warning=FALSE, message=FALSE, fig.align='center', fig.height = 4, fig.width = 4}
edaPlots(dataset_updated_train, output_type = "correlation", n_cols = 3)
```

### 7.1.2 Test Distribution {.tabset}

#### Summary Table
```{r}
edaPlots(dataset_updated_test, output_type = "summary", n_cols = 3)
```

#### Histograms
```{r histtest, figures-side, fig.show="hold", fig.height=2.2, fig.width =2.95, warning=FALSE, message=FALSE}
edaPlots(dataset_updated_test, output_type = "histogram", n_cols = 3)
```

#### Box Plots
```{r  boxxtest, figures-side, fig.show="hold",fig.height=2.2, fig.width = 2.95,warning=FALSE, message=FALSE}
edaPlots(dataset_updated_test, output_type = "boxplot", n_cols = 3)
```

#### Correlation Plot
```{r corrtest, warning=FALSE, message=FALSE, fig.align='center', fig.height = 4, fig.width = 4}
edaPlots(dataset_updated_test, output_type = "correlation", n_cols = 3)
```



# 8. Model Training

In HVT, we use `sammons` from the package `MASS` to project higher
dimensional data to a 2D space. The function `hvq` called from the `trainHVT`
function returns hierarchical quantized data which will be the input for
construction of the tessellations. The data is then represented in 2D
coordinates and the tessellations are plotted using these coordinates as
centroids. We use the package `deldir` for this purpose. The `deldir`
package computes the Delaunay triangulation (and hence the Dirichlet or
Voronoi tessellation) of a planar point set according to the second
(iterative) algorithm of Lee and Schacter. For subsequent levels,
transformation is performed on the 2D coordinates to get all the points
within its parent tile. Tessellations are plotted using these
transformed points as centroids. The lines in the tessellations are
chopped in places so that they do not protrude outside the parent
polygon. This is done for all the subsequent levels.

Let us try to understand the trainHVT function first.

```         
trainHVT(
  dataset,
  min_compression_perc,
  n_cells,
  depth,
  quant.err,
  normalize = TRUE,
  distance_metric = c("L1_Norm", "L2_Norm"),
  error_metric = c("mean", "max"),
  quant_method = c("kmeans", "kmedoids"),
  projection.scale,
  dim_reduction_method = c("sammon" , "tsne" , "umap")
  diagnose = FALSE,
  hvt_validation = FALSE,
  train_validation_split_ratio,
  tsne_perplexity,tsne_theta,tsne_verbose,
  tsne_eta,tsne_max_iter,
  umap_n_neighbors,umap_min_dist
)
```

Each of the parameters of trainHVT function have been explained below:

* __`dataset`__ - A dataframe, with numeric columns (features) that will be used for training the model.

* __`min_compression_perc`__ - An integer, indicating the minimum compression percentage to be achieved for the dataset. It indicates the desired level of reduction in dataset size compared to its original size.

* __`n_cells`__  - An integer, indicating the number of cells per hierarchy (level). This parameter determines the granularity or level of detail in the hierarchical vector quantization.

* __`depth`__   - An integer, indicating the number of levels. A depth of 1 means no hierarchy (single level), while higher values indicate multiple levels (hierarchy).

* __`quant.err`__ - A number indicating the quantization error threshold. A cell will only breakdown into further cells if the quantization error of the cell is above the defined quantization error threshold.

* __`normalize`__	- A logical value indicating if the dataset should be normalized. When set to TRUE, scales the values of all features to have a mean of 0 and a standard deviation of 1 (Z-score)

* __`distance_metric`__	- The distance metric can be `L1_Norm`(Manhattan) or `L2_Norm`(Euclidean). `L1_Norm` is selected by default. The distance metric is used to calculate the distance between an `n` dimensional point and centroid. 

* __`error_metric`__ - The error metric can be `mean` or `max`. `max` is selected by default. `max` will return the max of `m` values and `mean` will take mean of `m` values where each value is a distance between a point and centroid of the cell.

* __`quant_method`__ - The quantization method can be `kmeans` or `kmedoids`. Kmeans uses means (centroids) as cluster centers while Kmedoids uses actual data points (medoids) as cluster centers. `kmeans` is selected by default.

* __`projection.scale`__ - A number indicating the scale factor for the tessellations to visualize the sub-tessellations well enough. It helps in adjusting the visual representation of the hierarchy to make the sub-tessellations more visible. Default is 10.

* __`dim_reduction_method`__ - The dimensionality reduction method to be chosen. options are 'tsne' , 'umap' & 'sammon'. Default is 'sammon'. 

* __`scale_summary`__ -  A list with user defined mean and standard deviation values for all the features in the dataset. Pass the scale summary when normalize is set to FALSE.
    
* __`diagnose`__ - A logical value indicating whether user wants to perform diagnostics on the model. Default value is FALSE. 

* __`hvt_validation`__ - A logical value indicating whether user wants to holdout a validation set and find mean absolute deviation of the validation points from the centroid. Default value is FALSE.

* __`train_validation_split_ratio`__ - A numeric value indicating train validation split ratio. This argument is only used when hvt_validation has been set to TRUE. Default value for the argument is 0.8

* __`tsne_perplexity`__ - A numeric, balances the attention t-SNE gives to local and global aspects of the data. Lower values focus more on local structure, while higher values consider more global structure. It is recommended to be between 5 and 50. Default value is 30.


* __`tsne_theta`__ - A numeric, speed/accuracy trade-off parameter for Barnes-Hut approximation. If set to 0, exact t-SNE is performed, which is slower. If set to greater than 0, an approximation is used, which speeds up the process but may reduce accuracy. Default value is 0.5


* __`tsne_eta (learning_rate)`__ - A numeric, learning rate for t-SNE optimization.Determines the step size during optimization. If too low, the algorithm might get stuck in local minima; if too high, the solution may become unstable. Default value is 200.

* __`tsne_max_iter`__ - An integer, maximum number of iterations. Number of iterations for    the optimization process. More iterations can improve results but increase computation time. Default value is 1000.

* __`umap_n_neighbors`__  - An integer, the size of the local neighborhood (in terms of number of neighboring sample points) used for manifold approximation, controls the balance  between local and global structure in the data, smaller values focus on local structure, while larger values capture more global structures. Default value is 15.

* __`umap_min_dist`__  - A numeric, the minimum distance between points in the embedded space, controls how tightly UMAP packs points together, lower values result in a more clustered embedding. Default value is 0.1



The output of trainHVT function (list of 7 elements) have been explained below with an image attached for clear understanding.

**NOTE: Here the attached image is the snapshot of output list generated from model training which can be referred later in this section**

```{r trainhvt list,echo=FALSE,warning=FALSE,fig.show='hold',message=FALSE,fig.cap='Figure 3: The Output list generated by trainHVT function.', out.width="50%", out.height=  "20%", fig.align='center'}
knitr::include_graphics('./pngs/hvt_result_diag.png')
```

* The '1st element' is a list containing information related to plotting tessellations. This information might include coordinates, boundaries, or other details necessary for visualizing the tessellations

* The '2nd element' is a list containing information related to Sammon's projection coordinates of the data points in the reduced-dimensional space.

* The '3rd element'  is a list containing detailed information about the hierarchical vector quantized data along with a summary section containing no of points, Quantization Error and the centroids for each cell for 2D.

* The '4th element'  is a list that contains all the diagnostics information of the model when diagnose is set to TRUE. Otherwise NA.

* The '5th element' is a list that contains all the information required to generates a Mean Absolute Deviation (MAD) plot, if hvt_validation is set to TRUE. Otherwise NA 

* The '6th element'  is a list containing detailed information about the hierarchical vector quantized data along with a summary section containing no of points, Quantization Error and the centroids for each cell which is the output of `hvq`.

* The ‘7th element’ (model info) is a list that contains model generated time, input parameters passed to the model, validation results and the dimensionality reduction evaluation metrics table.



More information on building an HVT model at different levels and
visualizing the output can be found
<a href="https://nbviewer.org/github/Mu-Sigma/HVT/blob/master/vignettes/HVT_vignette.html" target="_blank">here</a>.

In the section below, we build a Level 1 HVT model. The number of
cells (`n_cells`) is set to 500.

```{r level one sample data,warning=FALSE,message=FALSE,results='asis'}
hvt.results <-trainHVT(dataset_updated_train,
                          n_cells = 500,
                          depth = 1,
                          quant.err = 0.1,
                          normalize = FALSE,
                          distance_metric = "L2_Norm",
                          error_metric = "max",
                          quant_method = "kmeans",
                          diagnose = TRUE,
                          hvt_validation = TRUE,
                          train_validation_split_ratio=0.8,
                          dim_reduction_method = "sammon")
```


```{r}
displayTable(hvt.results[[3]][['compression_summary']])
```

As seen in the above table, 90% of the cells have a quantization error
below the threshold.

Let's have a closer look at `Quant.Error` of the cells. Here we are
showing just top 100 rows for the sake of brevity.

```{r}
displayTable(data =hvt.results[[3]][['summary']])
```






Let's take a look at the 2D HVT plot.

```{r fig.height=4, fig.width=6,fig.align='center', warning=FALSE, message=FALSE}
plotHVT(hvt.results,
        line.width = c(0.6 ),
        color.vec = c("black"),
        centroid.size = 1,
        maxDepth = 1, 
        plot.type = '2Dhvt')
```

## 8.1 Model Diagnostics and Validation

### 8.1.1 Diagnostics {.unlisted .unnumbered}

**HVT model diagnostics** are used to evaluate the model fit and
investigate the proximity between centroids. The distribution of
proximity value can also be used to decided an optimum Mean Absolute
Deviation threshold for HVT model based scoring.

The diagnosis can be enabled by setting the **`diagnose`** parameter to
`TRUE` while building the HVT Model.

### 8.1.2 Validation {.unlisted .unnumbered}

**Model validation** is used to measure the fit/quality of the model.
Measuring model fit is the key to iteratively improving the models. The
relevant measure of model quality here is percentage of anomalous
points. The percentage anomalies ideally should match with level of
compression achieved during modeling, where PercentageAnomalies $\approx$
1-ModelCompression.

Model Validation can be enabled by setting the **`hvt_validation`**
parameter to `TRUE` and setting the `train_validation_split_ratio` value
while training the HVT Model.

The model trained above has a `train_validation_split_ratio` of 0.8, i.e
80% of the Train dataset is used for training the model while the
remaining 20% will be used for validation

**Note:** User can skip this step, if the number of observations in
train data is low.

## 8.2 Diagnostic Plots

The basic tool for examining the model fit is proximity plots and
distribution of observations across centroids.

The proximity between object can be measured as **distance matrix**. The
distances between the objects is calculated using Manhattan or Euclidean
Distance and put into a matrix form. In the next step we find the
**minimum value for each row**, excluding the diagonal values, as the
diagonal elements of distance matrix are zero representing distance from
an object to itself. This minimum distance value gives the
**proximity(distance to nearest neighbour)** of other object in the
datatable.

``plotModelDiagnostics()` function can be used to print diagnostic plots for HVT model
or HVT scoring.

The `plotModelDiagnostics()` function for HVT Model provides 5 diagnostic plots which
are as follows:

-   Mean Absolute Deviation Plot: Calibration: HVT Model \| Train Data
-   Minimum Intra-DataPoint Distance Plot: Train Data
-   Minimum Intra-Centroid Distance Plot: HVT Model \| Train Data
-   Distribution of Number of Observations in Cells: HVT Model \| Train
    Data
-   Singletons Pie Chart: HVT Model \| Train Data

Let's have look at the function `plotModelDiagnostics` which we will use to print
the diagnostic plots.

::: superbigimage
```{r diagplot function1,echo = TRUE, eval= TRUE, fig.height=12, fig.width=14,class.source = 'fold-show', warning=FALSE, message=FALSE}
plotModelDiagnostics(hvt.results)
```
:::

### 8.2.1 Mean Absolute Deviation based Calibration: HVT Model \| Train Data

The first diagnostic plot is a calibration plot for HVT Model run on
train data. This plot is obtained by executing the train data
itself on the HVT model. It is a comparison of Percentage_Anomalies with
at varying Mean Absolute Deviation values. It can be seen from the plot
that at **0.1** Mean Absolute Deviation value the percentage anomalies
drop below one percent.

```{r fig.height=4, fig.width=6,fig.align='center'}
p3=hvt.results[[4]]$mad_plot_train+ggtitle("Mean Absolute Deviation Plot: Calibration: HVT Model | Train Data")
p3
```

### 8.2.2 Minimum Intra-DataPoint Distance Plot: Train Data

The second diagnostics plot helps us in finding out how the points in
the training data are distributed. Shown below is a histogram of minimum
distances with nearest neighbour for each observation in train data.

```{r fig.height=4, fig.width=6,fig.align='center', warning=FALSE, message=FALSE}
p1=hvt.results[[4]]$datapoint_plot+ggtitle("Minimum Intra-DataPoint Distance Plot: Train Data")
p1
```

As seen in the plot above the mean value is **0.02**.

### 8.2.3 Minimum Intra-Centroid Distance Plot: HVT Model \| Train Data

The third diagnostics plot helps us in finding out how the centroids in
the HVT Model are distributed. Shown below is a histogram of minimum
distances with nearest neighbour for each centroid in HVT Model

```{r fig.height=4, fig.width=6,fig.align='center', warning=FALSE, message=FALSE}
p2=hvt.results[[4]]$cent_plot+ggtitle("Minimum Intra-Centroid Distance Plot: HVT Model | Train Data")
p2
```

<font color="red"> As seen in the plot above the mean value is **0.8**.
This value can be selected as the Mean Absolute Deviation Threshold for
scoring data using scoreHVT function </font>

### 8.2.4 Distribution of Number of Observations in HVT Cells

The fourth diagnostics plot finds out distribution of number of
observations in each centroid. Shown below is a histogram to depict the
same.

```{r fig.height=4, fig.width=6,fig.align='center'}
p4=hvt.results[[4]]$number_plot+ggtitle("Distribution of Number of Observations in Cells: HVT Model | Train Data")
p4
```

As shown in the plot above the mean number of records in each HVT cell
is **15**.

### 8.2.5 Singleton Count

The fifth diagnostics plot finds out number of Singleton centroids
(Segments/Centroids with single observation.)

```{r fig.height=4, fig.width=6,fig.align='center'}
p5=hvt.results[[4]]$singleton_piechart
p5
```

**Validation**

The Mean Absolute Deviation Plot for Validation Data has been shown in
above section. Alternatively to fetch it separately, we can use the
following code:

```{r fig.height=4, fig.width=6,fig.align='center'}
m1=hvt.results[[5]][["mad_plot"]]+ggtitle("Mean Absolute Deviation Plot:Validation")
m1
```

As seen in the plot the mean absolute deviation for the validation data from the given `dataset_updated_train` is **0.11**.

# 9. Scoring

Now once we have built the model, let us try to score using our test
dataset to see which cell each point belongs to.

## 9.1 Scoring Algorithm

The Scoring algorithm recursively calculates the distance between
each point in the test dataset and the cell centroids for each level.
The following steps explain the scoring method for a single point in
the test dataset :

1.  Calculate the distance between the point and the centroid of all the
    cells in the first level.
2.  Find the cell whose centroid has minimum distance to the point.
3.  Check if the cell drills down further to form more cells.
4.  If it doesn't, return the path. Or else repeat steps 1 to 4 till we
    reach a level at which the cell doesn't drill down further.
    
## 9.2 Load Test Data

The user can provide an absolute or relative path in the cell below to
access the test data from his/her computer.

```{r loading csv test, message=FALSE, warning=FALSE,class.source = 'fold-show'}
load_test_data=FALSE
if(load_test_data){
  file_name <- " " #enter the name of the local file for validation
  file_path <- " " #enter the path of the local file for validation
  file_load <- paste0(file_path, file_name)
  dataset_updated_test <- as.data.frame(fread(file_load))
  
  if(nrow(dataset_updated_test) > 0){


    paste0("File ", file_name, " having ", nrow(dataset_updated_test), " row(s) and ",
ncol(dataset_updated_test), " column(s)",  " imported successfully. ") %>% cat("\n")
dataset_updated_test <- dataset_updated_test %>% mutate_if(is.numeric, round, digits = 4)
    paste0("Code chunk executed successfully. Below table showing first 10 row(s) of the dataset.") %>% cat("\n")
    dataset_updated_test %>% head(10) %>%as.data.frame() %>%DT::datatable(options = options, rownames = TRUE)
  }
  
  colnames( dataset_updated_test) <- colnames( dataset_updated_test) %>% casefold()
 dataset_updated_test <- spaceless( dataset_updated_test)
}
```

### 9.2.1 Transformation of Categorical Features

In this section we will perform one hot encoding on test dataset, based
on whether one hot encoding has been performed on train dataset.

```{r dummify_Test, purl=FALSE,eval = dummify_cat,class.source = 'fold-show'}

if(dummify_cat){
dummified_cols_test <- dataset_updated_test %>% dplyr::select(dum_cols) %>%
  dummies::dummy.data.frame(dummy.classes = "ALL", sep = "_")

names(dummified_cols_test) <- gsub(pattern = "[^[:alnum:]]+",
                             replacement = ".",
                             names(dummified_cols_test))

columns_difference=setdiff(dummified_cols,names(dummified_cols_test))
dummified_cols_test[,columns_difference]=0

dataset_updated_test <- dataset_updated_test %>% cbind(dummified_cols_test) %>%     
# append encoded columns
dplyr::select(-dum_cols)
# remove the old categorical columns
dummified_cols_test %>% head(5) %>% DT::datatable(options = options)
}

```

### 9.2.2 Subsetting Test Dataset

In this section we will subset the test data based on numeric columns
present in train data.

```{r, purl = FALSE,results='asis',class.source = 'fold-show', warning=FALSE, message=FALSE}
dataset_updated_test=dataset_updated_test %>% dplyr::select(nums)
```

Now once we have the test data ready, lets look at how the scoreHVT
function looks like.

## 9.3 Scoring on Test Data

```         
scoreHVT(dataset,
         hvt.results,
         child.level,
         mad.threshold,
         line.width,
         color.vec,
         normalize,
         distance_metric,
         error_metric,
         yVar,
         analysis.plots, 
         names.column)
```

The important parameters for the function `scoreHVT` are as below

- **`dataset`** - A dataframe containing the test dataset. The dataframe should have all the variable(features) used for training. 

-  **`hvt.results.model`** - A list obtained from the trainHVT function while performing hierarchical vector quantization on training data. This list provides an overview of the hierarchical vector quantized data, including diagnostics, tessellation details, Sammon's projection coordinates, and model input information.

-  **`child.level`** - A number indicating the depth for which the heat map is to be plotted. Each depth represents a different level of clustering or partitioning of the data.

- **`mad.threshold`**  - A numeric value indicating the permissible Mean Absolute Deviation which is obtained from Minimum Intra centroid plot(when diagnose is set to TRUE in trainHVT). `mad.threshold` value is important since it is used in anomaly detection. Default value is 0.2
*NOTE: for a given datapoint, when the quantization error is above `mad.threshold` it is denoted as anomaly else not.*

- **`line.width`**	- A vector indicating the line widths of the tessellation boundaries for each layer. (Optional Parameter)

* **`color.vec`**	- A vector indicating the colors of the tessellations boundaries at each layer. (Optional Parameter)

* **`normalize`** - A logical value indicating if the dataset should be normalized. When set to TRUE, the data (testing dataset) is standardized by mean and sd of the training dataset referred from the trainHVT(). When set to FALSE, the `data` is used as such without any changes.

* **`distance_metric`** - The distance metric can be `L1_Norm`(Manhattan) or `L2_Norm`(Euclidean). The metric is used when calculating distance between each datapoint(in test dataset) with the centroids obtained from results of trainHVT. Default is `L1_Norm`.

* **`error_metric`** - The error metric can be `mean` or `max`.  `max` will return the max of `m` values and `mean` will take mean of `m` values where each value is a distance between the datapoint and centroid of the cell. This helps in calculating the scored quantization error. Default value is `max`.

* **`yVar`** - A character or a vector representing the name of the dependent variable(s)

*The below given arguments are used only when character column can be mapped over the scored results. since torus doesn't have a character column, we are not using them in this vignette.*

* **`analysis.plots`** - A logical value to indicate whether to include the insight plots which are useful in viewing the contents and clusters of cells. Default is FALSE.

* **`names.column`** - The column of names of the datapoints which will be displayed as the contents of the cell in 'scoredPlotly'. Default is NULL.




<p style="color:red;">

Here the mad_threshold has been selected as 0.8 which is based on Mean
of Minimum Intra-Centroid Distance plot from above.
</p>

```{r,class.source = 'fold-show', warning=FALSE, message=FALSE, results='asis'}
hvt.score <- scoreHVT(dataset_updated_test,
                      hvt.results,
                      child.level = 1,
                      mad.threshold = 0.8, 
                      line.width = c(0.6, 0.4, 0.2),
                      color.vec = c("navyblue", "slateblue", "lavender"),
                      distance_metric = "L1_Norm",
                      error_metric = "max")
```


```{r}
displayTable(hvt.score[["scoredPredictedData"]], value = 0.8)
```



The `plotModelDiagnostics()` function can be called for scoring object as well.
Shown below is the comparison of Mean Absolute Deviation Plot for train
data and test data.

::: superbigimage
```{r, fig.width=8,fig.height=10,class.source = 'fold-show',fig.align='center', warning=FALSE, message=FALSE}
plotModelDiagnostics(hvt.score)
```
:::



### 9.3.1 Quantization Error Comparison (Train vs. Test)

Table below shows cell(s) containing anomalous test data points.
Datapoints are flagged as anomalous when Quantization Error for such is
greater than the same of assigned centroid based on error metric.
Comparison between scored/test and fitted Quantization error of cell(s)
is provided for further insights.

```{r out.width="100%", echo=FALSE}
QEdata=hvt.results[[3]]$summary

Quant.Error.Actual <- QEdata %>% 
      mutate(Quant.Error =  Quant.Error * n) %>% 
      select(Quant.Error, n) %>% 
      summarise_all(sum) %>% 
      transmute(Quant.Error = Quant.Error/ n) %>% 
      unlist()%>%round(4)

QECompareDf <- hvt.score$QECompareDf %>% filter(anomalyFlag == 1)
percentageAnomalies = formatC((sum(QECompareDf$n) / nrow(dataset_updated_test)) *
                                100, digits = 2, format = "f") %>% paste0("%")
```

**Number of test data points: `r nrow(dataset_updated_test)` \| Number of
anomalous data points: `r sum(QECompareDf$n)` \| Percentage of anomalous
data points: `r percentageAnomalies`** <br> **Mean QE for fitted data:
`r Quant.Error.Actual %>% round(digits = 4)` \| Mean QE for test data:
`r hvt.score[["scoredPredictedData"]]$Quant.Error %>%  mean() %>% round(digits = 4)`
\| Difference in QE between fitted and test data:
`r abs(Quant.Error.Actual%>%round(digits = 4)-hvt.score[["scoredPredictedData"]]$Quant.Error%>%mean() %>%round(digits = 4))`**

```{r fig.height=7.5, fig.width=6,fig.align='center', warning=FALSE, message=FALSE}
plotQuantErrorHistogram(hvt.results,hvt.score)
```

### 9.3.2 Anomalous Observations

The anomalous observations are shown below in the datatable.

NOTE: **Since there are no anomaly found in torus dataset that we use, the table below will be empty**.


```{r,echo=TRUE,eval=TRUE, warning=FALSE, message=FALSE}
QECompareDf <- hvt.score$QECompareDf %>% filter(anomalyFlag == 1)
displayTable(QECompareDf)
```


# 10. Applications

1.  Pricing Segmentation - The package can be used to discover groups of
    similar customers based on the customer spend pattern and understand
    price sensitivity of customers

2.  Market Segmentation - The package can be helpful in market
    segmentation where we have to identify micro and macro segments. The
    method used in this package can do both kinds of segmentation in one
    go

3.  Anomaly Detection - This method can help us categorize system
    behaviour over time and help us find anomaly when there are changes
    in the system. For e.g. Finding fraudulent claims in healthcare
    insurance

4.  The package can help us understand the underlying structure of the
    data. Suppose we want to analyze a curved surface such as sphere or
    vase, we can approximate it by a lot of small low-order polygons in
    the form of tessellations using this package

5.  In biology, Voronoi diagrams are used to model a number of different
    biological structures, including cells and bone microarchitecture

6.  Using the base idea of Systems Dynamics, these diagrams can also be
    used to depict customer state changes over a period of time

# 11. References

1. <a href="https://users.ics.aalto.fi/jhollmen/dippa/node9.html" target="_blank">Topology Preserving Maps</a>

2. <a href="https://ocw.mit.edu/courses/6-450-principles-of-digital-communications-i-fall-2006/resources/book_3/" target="_blank">Vector Quantization</a>

3. <a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank">K-means</a>

4. <a href="https://en.wikipedia.org/wiki/Sammon_mapping" target="_blank">Sammon's Projection</a>

5. <a href="https://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation" target="_blank">Voronoi Tessellations</a>
