---
title: "muHVT: Collection of functions used for clustering and construction
of hierarchical Voronoi Tessellations for data analysis in R"
author: "Meet Dave"
date: "`r Sys.Date()`"
output: 
      rmarkdown::html_document:
      number_sections: true
        
vignette: >
  %\VignetteIndexEntry{muHVT Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8, 
  fig.height = 5, 
  fig.align = "center",
  dpi = 96
)

library(muHVT)
set.seed(420)
```


# Abstract


The muHVT package is a collection of R functions for clustering and
construction of hierarchical voronoi tessellations as a data visualization
tool to visualize clusters using quantization. The hierarchical clusters are computed using
Hierarchical K-means where a quantization threshold governs the levels
in the hierarchy for a set $k$ parameter (the maximum number of clusters
at each level). The LBG Vector Quantization (LBG VQ) algorithm is also
implemented which detects the number of clusters for level one, based on
a specified quantization threshold.The package is helpful to visualize rich mutlivariate data. 


This package additionally provides functions for computing the
Jensen-Shannon-Bregman Divergence, Sammon’s projection and plotting the
heat map of the variables on the tiles of the tessellations.


#Clustering


This package supports clustering using two algorithms- 

1. Hierarchical k-means
1. LBG Vector Quantization

The hierarchical k-means should be selected if the number of clusters are known beforehand. LBG Vector Quantization detects the number of clusters based on a specific quantization threshold. 

##Hierarchical k-means

### k-means clustering

1. The k-means algorithm randomly selects *k* data points as initial means.
1. *k* clusters are formed by assigning each data point to its closest cluster mean. The algorithm uses the Euclidean distance.
1. Virtual means for each cluster are calculated by using all datapoints contained in a cluster.


The second and third step is iterated until a predefined number of iteration is reached or the clusters converge. The runtime for the algorithm is O(n).

### Hierarchical k-means clustering

The algorithm divides the dataset recursively into clusters. The $k-means$ algorithm is used by setting $k$ to say, two in order to divide the dataset into two subsets.Then, the two subsets are divided again into two subsets by setting $k$ to two to result in a total of four subsets. The recursion terminates when the clusters contain single data points or a stop criterion is reached. In this case, the stop criterion is when the cluster error exceeds the quantization threshold.
The cluster error is the Mean Absolute Percentage Error (MAPE) :
$$M = \frac{100\%}{n}\sum_{t=1}^n|\frac{A_t-F_t}{A_t}|$$ 
where $A_t$ is the centroid and $F_t$ is the data point in the cluster. $n$ is the number of points in the cluster.


The difference between $A_t$ and $F_t$ is divided by the Actual value $A_t$ again. The absolute value in this calculation is summed for every fitted or forecasted point in time and divided again by the number of fitted points $n$. Multiplying by 100 makes it a percentage error.

## LBG Vector Quantization


The Linde Buzo Gray design algorithm is an iterative algorithm which alternatively solves the two optimality criteria:

1. Nearest Neighbor Condition
$$S_n=\{x:||x-c_n||^2\leq||x-c_{n'}||^2  \forall n' = 1,2,.., N\}$$

2. Centroid Condition
$$c_n = \frac{\sum_{x_m \in S_n}x_m}{\sum_{x_m \in S_n}1} ;n= 1,2,.. N$$


The algorithm requires an initial codebook $c_{0}$. This initial codebook is obtained by the splitting method. In this method, an initial codevector is set as the average of the entire training sequence. This codevector is then split into two. The iterative algorithm is run with these two vectors as the initial codebook. The final two codevectors are split into four and the process is repeated until the desired number of codevectors is obtained. The algorithm is summarized below.

###LBG Design Algorithm:

We assume that there is a training sequence consisting of *M* source vectors: 

$$\tau = \{x_1, x_2, ..., x_M\}$$ 
Let *N* be the number of codevectors and let 

$$c=\{c_1, c_2, ..., c_N\}$$ 

1. Given $\tau$, fix $\epsilon > 0$ ; the quantization threshold, to be a ‘small’ number.

2. Let $N=1$ and 
$$c_1^*= \frac{1}{M}\sum_{m=1}^M x_m$$ 
calculate squared-error distortion measure
$$D_{ave}^*= \frac{1}{Mk}\sum_{m=1}^M||x_m - c_1^*||^2$$ 

3. **Splitting**: For $i=1,2..N$, set 
$$c_i^0 = (1+\epsilon) c_i^*$$
$$c_{N+i}^0 = (1-\epsilon)c_i^*$$ 

4. **Iteration**: Let
$D_{ave}^0=D_{ave}^*$. Set the iteration index $i=0$.

    i. For $m=1,2, ..., M$, find the minimum value of 
    $$||x_m-c_n^i||^2$$
    over all $n=1,2,..,N$. Let $n^*$ be the index which achieves the
    minimum. Set 
    $$Q(x_m)=c_{n^*}^i$$
    
    ii. For $n=1,2,..,N$, update the codevector
    $$c_n^{(i+1)}= \frac{\sum_{Q(x_m)=c_n^i}x_m}{\sum_{Q(x_m)=c_n^i}1}$$

    iii. Set $i=i+1$

    iv. Calculate 
    $$D_{ave}^i=\frac{1}{Mk}\sum_{m=1}^M ||x_m-Q(x_m)||^2$$

    v. If $(D_{ave}^{(i-1)}-D_{ave}^{(i)}/D_{ave}^{(i-1)}>\epsilon$, go backto Step (i).

    vi. Set $D_{ave}^*=D_{ave}^{(i)}$. For $n=1,2,.., N$ set 
    $$c_n^*=c_n^{(i)}$$ 
    as the final codevectors.
    
5. Repeat Steps 3 and 4 until the desired number of codevectors is obtained.


#Voronoi Tessellations

A Voronoi diagram is a way of dividing space into a number of regions. A set of points (called seeds, sites, or generators) is specified beforehand and for each seed there will be a corresponding region consisting of all points closer to that seed than to any other. The regions are called Voronoi cells. It is dual to the Delaunay triangulation.

##Sammon’s projection

Sammon projection is an algorithm that maps a high-dimensional space to a space of lower dimensionality by trying to preserve the structure of inter-point distances in high-dimensional space in the lower-dimension projection. It is particularly suited for use in exploratory data analysis. It is considered a non-linear approach as the mapping cannot be represented as a linear combination of the original variables. The centroids are plotted in 2D after performing Sammon’s projection at every level of the tessellation.


Denote the distance between $i^{th}$ and $j^{th}$ objects in the original space by $d_{ij}^*$, and the distance between their projections by $d_{ij}$. Sammon’s mapping aims to minimize the following error function, which is often referred to as Sammon’s stress or Sammon’s error:
$$E=\frac{1}{\sum_{i<j} d_{ij}^*}\sum_{i<j}\frac{(d_{ij}^*-d_{ij})^2}{d_{ij}^*}$$
The minimization can be performed either by gradient descent, as proposed initially, or by other means, usually involving iterative methods. The number of iterations need to be experimentally determined and convergent solutions are not always guaranteed. Many implementations prefer to use the first Principal Components as a starting configuration.

###Example Usage

In this section, we will work with Uber dataset, which contains data generated by Uber for the city of New York. Uber Technologies Inc. is a peer-to-peer ride sharing platform. The data is freely available on [Kaggle](https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city/data). The dataset contains raw data on Uber pickups with information such as the date, time of the trip along with longitude-latitude information. In this case, we are taking data for the month of April 2014.

#### New York city has five boroughs: 

* Brooklyn 
* Queens 
* Manhattan
* Bronx
* Staten Island. 

In this example, we will try to identify these boroughs, areas within these boroughs by applying hierarchical k-means clustering and visualize the clusters by constructing hierarchical voronoi tesselation.

```{r load data,warning=FALSE,message=FALSE}

set.seed(420)

# Load data from csv files
apr14 <- read.csv("https://raw.githubusercontent.com/fivethirtyeight/uber-tlc-foil-response/master/uber-trip-data/uber-raw-data-apr14.csv")


# Bind the data
uber_data <- apr14

```

Let's get a summary of the data

```{r data summary,warning=FALSE,message=FALSE}
summary(uber_data)
```
In this example, we will consider only longitude and latitude data. The number of clusters will be 5- the number of boroughs in NYC.

First we will perform hierarchical clustering at level 1 by setting the parameter depth to 1. Here level 1 signifies no hierarchy.

```{r level one,warning=FALSE,message=FALSE,results='asis'}

# Subsetting latitude and longitude data. 
set.seed(420)

uber_data <- uber_data[,2:3]

hvt.results <- list()

# We know beforehand that NewYork has five boroughs. Hence no of cluster is 5. 
# depth=1 is used for level1 in the hierarchy

hvt.results <- muHVT::HVT(uber_data,nclust = 5,depth = 1,quant.err = 0.001,projection.scale = 10,normalize = F)

```

Let's plot the voronoi tesselation

```{r plot level one,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 1: The Voronoi tessellation for level 1 shown for the 5 clusters in the dataset ’uber’'}

# Voronoi tesselation plot for level one

plotHVT(hvt.results,line.width = c(1.5), color.vec = c("#326273"))

```

As per the manual, **hvt.results[[3]]** gives us detailed information about the hierarchical vector quantized data

**hvt.results[[3]][['summary']]** gives a nice tabular data containing no of points, quantization error, centroids for each cluster at every level

```{r summary level one,warning=FALSE}

hvt.results[[3]][['summary']]

```

Now that we have obtained the five clusters, let's look at their centroids

```{r centroids level one,warning=FALSE,eval=FALSE}

hvt.results[[3]][['summary']][,6:7]

```

Let's do reverse geo-coding and find out where these centroids belong.

 | Latitude| Longitude| Place|
   |-|-|-|
   |40.65962|-73.77410|Queens|
   |40.72977|-74.00318|Manhattan|
   |40.68855|-73.96567|Brooklyn|
   |40.76467|-73.97357|Manhattan|
   |40.79817|-73.86941|Bronx|

As it can be seen, we were successfully able to identify 4 boroughs out of 5.

Now let's go one level deeper and perform hierarchical clustering.

```{r level two,warning=FALSE,message=FALSE,results="hide"}

# We will consider only latitude and longitude data. 
set.seed(420)

hvt.results2 <- list()

# depth=2 is used for level2 in the hierarchy

hvt.results2 <- muHVT::HVT(uber_data,nclust = 5,depth = 2,quant.err = 0.001,projection.scale = 10,normalize = F)

```

#### Let's plot the voronoi tesselation for both the levels.

```{r plot level two,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 2: The Voronoi tessellation for level 2 shown for the 5 clusters in the dataset ’uber’'}

# Voronoi tesselation plot for level two

plotHVT(hvt.results2, line.width = c(1.5, 0.8), color.vec = c("#326273","#ADB2D3"))

```

In the table below, Segment Level signifies the depth.

Level 1 has 5 clusters

Level 2 has 25 clusters.i.e. each cluster in level1 is divided into 5 clusters


```{r summary level two,warning=FALSE}

hvt.results2[[3]][['summary']]

```

Let's get the centroids for level two.

```{r centroids level two,warning=FALSE}

hvt.results2[[3]][['summary']][6:30,c(1,2,3,6,7)]

```

Now let's reverse geo-code these centroids and find out which neighbourhood in the NYC borough do they belong to.

For the sake of brevity,  I will randomly select 5 centroids and show the reverse geo-codes.


 Latitude| Longitude| Neighbourhood|Place|
   |-|-|-|-|
   |40.77111|-73.86998|Queens| Elmhurst|
   |40.86723|-73.89915|Bronx|West Bronx|
   |40.71540|-73.95467|Brooklyn|Williamsburg|
   |40.75032|-73.98074|Manhattan|Midtown|
   |40.76449|-73.98723|Manhattan|Hell's Kitchen|
   
The reverse geo-code mapping can be checked [here](https://www.latlong.net/Show-Latitude-Longitude.html)

![](../revgeocode.png)


Similary we can repeat the process for level 3.

```{r level three,warning=FALSE,message=FALSE,results="hide"}

# We will consider only latitude and longitude data. 
set.seed(420)

hvt.results3 <- list()

# depth=2 is used for level2 in the hierarchy

hvt.results3 <- muHVT::HVT(uber_data,nclust = 5,depth = 3,quant.err = 0.001,projection.scale = 10,normalize = F)

```

Let's plot the voronoi tesselation for all 3 levels.

```{r plot level three,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 3: The Voronoi tessellation for level 3 shown for the 5 clusters in the dataset ’uber’'}

# Voronoi tesselation plot for level three

plotHVT(hvt.results3,line.width = c(1.5,0.8,0.2),color.vec = c("#326273","#ADB2D3","#BFA89E"))

```

Now we will try to get more insights from the clusters by overlaying heatmap for variable 'hour' at different levels.

Let's do it for level one

```{r hmp first level,warning=FALSE,results='hide',message=FALSE,eval=T}
set.seed(420)
library(lubridate)

data14 <- apr14

# Separate or mutate the Date/Time columns
data14$Date.Time <- mdy_hms(data14$Date.Time)
data14$Year <- factor(year(data14$Date.Time))
data14$Month <- factor(month(data14$Date.Time))
data14$Day <- factor(day(data14$Date.Time))
data14$Weekday <- factor(wday(data14$Date.Time))
data14$Hour <- factor(hour(data14$Date.Time))
data14$Minute <- factor(minute(data14$Date.Time))
data14$Second <- factor(second(data14$Date.Time))

d_u <- data14[,c(2,3,9)]
d_u <- purrr::map_df(d_u,as.numeric)

hvt.results <- muHVT::HVT(d_u,nclust = 5,depth = 1,quant.err = 0.001,projection.scale = 10,normalize = T)
```


####  Heatmap

In the plot below, a heatmap for variable 'hour' is overlayed on level one tesselation plot.We calculate the peak hour for each region and represent it as heatmap. For e.g it seems that Queen region (blue cluster) sees the most number of pick ups between **8AM -8:30AM**.

```{r hmp level one,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 4: The Voronoi tessellation with the heat map overlaid for variable ’hour’ in the ’uber’ dataset'}


gghvtHmap(hvt.results, d_u, child.level = 1,hmap.cols =c(3), show.points=TRUE,line.width = c(0.2),color.vec = c("#326273"),palette.color = 6)

```

Let's plot quantization error heatmap

```{r hmp quant level one,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 5: The Voronoi tessellation with the heat map overlaid for quantization error in the ’uber’ dataset'}


gghvtHmap(hvt.results, d_u, child.level = 1,hmap.cols =c(0), show.points=TRUE,line.width = c(0.2),color.vec = c("#326273"),palette.color = 6)

```

Let's plot heat map for no of points in a cluster

```{r hmp data points level one,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 5: The Voronoi tessellation with the heat map overlaid for quantization error in the ’uber’ dataset'}


gghvtHmap(hvt.results, d_u, child.level = 1,hmap.cols =c(-1), show.points=TRUE,line.width = c(0.2),color.vec = c("#326273"),palette.color = 6)

```

Now we will go one level deeper and do hierarchical k-means clustering for level 2. This should give us better insight about the peak hour for different neighbourhoods in a region.

```{r hmp third level,warning=FALSE,results='hide',message=FALSE,,eval=T}
set.seed(420)
hvt.results <- muHVT::HVT(d_u,nclust = 5,depth = 2,quant.err = 0.001,projection.scale = 10,normalize = T)
```

In the plot below, we have overlayed heatmap for variable 'hour' on level 2. 

The level 2 heatmap helps us visualize the peak hour distribution at a more granular level. In the Queen region, we can see that the peak hour for neighbourhoods varies from **3AM - 11AM**. We can also see that some of the neighbourhoods in other region have similar peak hour as the region level.

```{r hmp level three,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 5: The Voronoi tessellation with the heat map overlaid for the variable ’hour’ in the ’uber’ dataset'}


gghvtHmap(hvt.results, d_u, child.level = 2, hmap.cols = c(3),line.width = c(0.8,0.2),color.vec = c("#326273","#ADB2D3"),palette.color = 6,show.points=TRUE)

```
# Jensen-Shannon-Bregman Distance

The Jensen–Shannon divergence is a method of measuring the similarity between two probability distributions. It is also known as information radius (IRad) or total divergence to the average. The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen-Shannon distance.

Jensen-Shannon-Bregman distance $D_{x,y}$ between vectors $x$ and $y$ is defined as: 
$$D_{x,y} = \sqrt{JD_{x, y}}$$ 
where,
1. $x$, $y$: probability distributions (i.e., $\sum x$ = $\sum y$ = 1)
2. $JD_{x, y}$ is Jensen-Shannon-Bregman divergence, a symmetrized Bregman Divergence between non-negative vectors $x$ and $y$.
$$JD_{x, y} = 0.5 * (ID_{x, m} + ID_{y, m})$$ 
where,
$$m = 0.5 * (x + y)$$
I-divergence is defined as:
$$ID_{x, y} = \sum_{i=1}^N x_i \log (x_i / y_i) - \sum_{i=1}^N x_i + \sum_{i=1}^N y_i$$
The limitation with this measure is that the components of $x$ and $y$ vectors should be positive and non-zero.

## Applications 

The Jensen–Shannon divergence can be applied in bioinformatics and genome comparison in protein surface comparison, in the social sciences, in the quantitative study of history, and in machine learning

#References


1. Vector Quantization : http://www.data-compression.com/vq.html#lbg

2. Hierarchical k-means : http://gecco.org.chemie.uni-frankfurt.de/hkmeans/H-k-means.pdf

3. Jensen-Shannon-Bregman Divergence : http://machinelearning.wustl.edu/mlpapers/paper_files/BanerjeeMDG05.pdf

4. Sammon’s Projection : http://en.wikipedia.org/wiki/Sammon_mapping

5. Voronoi Tessellations : http://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation
