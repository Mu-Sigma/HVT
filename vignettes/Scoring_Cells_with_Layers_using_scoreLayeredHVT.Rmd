---
title: "HVT Scoring Cells with Layers using scoreLayeredHVT"
author: "Zubin Dowlaty, Srinivasan Sudarsanam, Somya Shambhawi"
date: "`r Sys.Date()`"
fig.height: 4
fig.width: 15
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth : 2
vignette: >
  %\VignetteIndexEntry{HVT: Scoring Cells with Layers using scoreLayerHVT}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{css, echo=FALSE}
/* CSS for floating TOC on the left side */
#TOC {
    /* float: left; */
    position: fixed;
    margin-left: -22vw;
    width: 18vw;
    height: fit-content;
    overflow-y: auto;
    padding-top: 20px;
    padding-bottom: 20px;
    background-color: #f9f9f9;
    border-right: 1px solid #ddd;
    margin-top: -14em; 
}

.main-container {
  margin-left: 222px; /* Adjust this value to match the width of the TOC + some margin */
}

li {
  padding-bottom: 5px;
}

```
```{=html}
<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>
```
```{r, warning = FALSE, include = FALSE,echo=FALSE}
# Sourcing required code modules for HVT
script_dir <- "../R"
r_files <- list.files(script_dir, pattern = "\\.R$", full.names = TRUE)
for (file in r_files) {
  source(file)
}


```


```{r, warning=FALSE, message=FALSE, include=FALSE, error=FALSE }

options=list(paging = TRUE,       # Paging of the data table
escape = FALSE, 
scrollX = TRUE,      # Horizontal scroll for the tables
searching = FALSE, # Search option at each column
lengthChange = FALSE,
      dom = 'prlti',
      initComplete = DT::JS(
        "function(settings, json) {",
        "$(this.api().table().header()).css({'font-size': '14px',
                                                        'background-color': '#8b9194', 'color': '#fff'});",
        "}"
      ),
      columnDefs = list(list(
        className = 'dt-center', targets = "_all"
      )),
      autowidth = TRUE
    )

# use where you need to show more than 10 rows of dataset
other_option <- list(
  escape = FALSE,
  scrollX = TRUE,
  paging = TRUE,
  pageLength = 10,
  searching = TRUE
)

# Counter Initialization
options(scipen=999)

table_no <- 0
fig_no <- 0
eq_no <- 0
ref_no <- 0

if(!require('devtools')){
  install.packages("devtools", repos = 'https://cloud.r-project.org/')
  library(devtools)
}else{
  library(devtools)
}

# for windows user
if (Sys.info()['sysname'] == 'Windows' && find_rtools() == FALSE){
  if(!require('installr')){
    install.packages("installr", repos = 'https://cloud.r-project.org/')
  }
  installr::install.URL(exe_URL = 'https://cloud.r-project.org/bin/windows/Rtools/Rtools35.exe')
}

library(knitr)
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)


row_keep=c("mean","std.dev","var","median","range","min","max","sum")



```

```{r lib, warning=FALSE, message=FALSE, include=FALSE, error=FALSE}
#Add the package name inside quotes '' to the list below  
# List of packages used in the notebook
list.of.packages <<- c(
  "plyr",
  "DT",
  'plotly',
  "magrittr",
  "data.table",
  "tidyverse",
  "crosstalk",
  "kableExtra",
  "gganimate",
      "gdata",
   "HVT",
  "data.table",
  "jmuOutlier",
  "viridis",
  "deldir",
  "conf.design",
    "splancs",
    "Hmisc",
    "xfun",
    "sp",
    "polyclip",
    "devtools",
    "deldir",
    "gdata",
    "tidyverse",
    "skimr",
      "patchwork",
  "crosstalk",
  "ggforce",
  "SmartEDA",
  "scales",
  "ggplot2",
  "htmlwidgets",
  "gridExtra",
  "gtable",
  "tibble"
)

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]

# Installing the missing packages
if (length(new.packages)){install.packages(new.packages, repos='https://cloud.r-project.org/')}

# Load all required packages
invisible(lapply(list.of.packages, library, character.only = TRUE))

```

```{r setup, warning = FALSE, include = FALSE,echo=FALSE}
# knitr::opts_chunk$set(
#   collapse = TRUE,
#   comment = "#>",
#   out.width = "672px",
#   out.height = "480px",
#   fig.width = 7,
#   fig.height = 5,
#   fig.align = "center",
#   fig.retina = 1,
#   dpi = 72
# )

knitr::opts_chunk$set(echo = TRUE,
                      collapse = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)


options=list(paging = TRUE,       # Paging of the data table
escape = FALSE, 
scrollX = TRUE,      # Horizontal scroll for the tables
searching = FALSE, # Search option at each column
lengthChange = FALSE,
      dom = 'prlti',
      initComplete = DT::JS(
        "function(settings, json) {",
        "$(this.api().table().header()).css({'font-size': '14px',
                                                        'background-color': '#8b9194', 'color': '#fff'});",
        "}"
      ),
      columnDefs = list(list(
        className = 'dt-center', targets = "_all"
      )),
      autowidth = TRUE
    )


# installing all required packages
list.of.packages <- c("dplyr", "kableExtra", "geozoo", "plotly", "purrr", "sp", "HVT")

new.packages <-
  list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
if (length(new.packages))
  install.packages(new.packages, dependencies = TRUE, repos='https://cloud.r-project.org/')

install.packages('https://cran.r-project.org/src/contrib/Archive/dummies/dummies_1.5.6.tar.gz', type="source", repos=NULL)
# Loading the required libraries
lapply(list.of.packages, library, character.only = TRUE)

options(expressions = 10000)



global_var <- nzchar(Sys.getenv("RUN_VIGNETTE"))
global_var <- TRUE

scrolLimit <- function(noOfRows){
  if(noOfRows<10){
    
    swe = paste(as.character(noOfRows*50),"px")
  }
  else{
    swe = "400px"
  }
  return(swe)
}

Table <- function(data,scroll = FALSE, limit = NULL){
  
  if(!is.null(limit)){
    data <- head(data,limit)
  }
  
  kable_table <- data %>% kable(escape = FALSE,align = "c") %>% kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
  
  scroll <- scroll
  
  if(scroll == TRUE){
  kable_table <- kable_table %>% scroll_box(width = "100%", height = scrolLimit(nrow(data)))
  }
  

  
  return(kable_table)
  
}

summaryTable <- function(data,scroll = TRUE,columnName='Quant.Error',value=0.2,limit=NULL){
  
  scroll <- scroll
  summaryTable <- data %>%  dplyr::mutate_if(is.numeric, funs(round(.,2))) %>% dplyr::mutate(!!columnName:=  cell_spec(eval(parse(text = columnName)),color = ifelse(is.na(eval(parse(text = columnName))),"#333",ifelse(eval(parse(text = columnName)) > value,"red","#333"))))  
  
  return(Table(summaryTable,scroll = scroll,limit = limit))
}

compressionSummaryTable <- function(data,scroll = TRUE,columnName='percentOfCellsBelowQuantizationErrorThreshold',value=0.8){
  summaryTable <- data %>%  dplyr::mutate_if(is.numeric, funs(round(.,2))) %>% dplyr::mutate(!!columnName:=  cell_spec(eval(parse(text = columnName)),color = ifelse(is.na(eval(parse(text = columnName))),"#00bb27",ifelse(eval(parse(text = columnName)) > value,"#00bb27","#333")))) 

  return(Table(summaryTable,scroll = scroll))
}



set.seed(240)
```

# 1. Abstract

The HVT package is a collection of R functions to facilitate building [topology preserving maps](https://users.ics.aalto.fi/jhollmen/dippa/node9.html) for rich multivariate data analysis. Tending towards a big data preponderance, a large number of rows. A collection of R functions for this typical workflow is organized below:

1.  **Data Compression**: Vector quantization (VQ), HVQ (hierarchical vector quantization) using means or medians. This step compresses the rows (long data frame) using a compression objective.

2.  **Data Projection**: Dimension projection of the compressed cells to 1D,2D or Interactive surface plot with the Sammons Non-linear Algorithm. This step creates topology preserving map (also called as [embedding](https://en.wikipedia.org/wiki/Embedding)) coordinates into the desired output dimension.

3.  **Tessellation**: Create cells required for object visualization using the Voronoi Tessellation method, package includes heatmap plots for hierarchical Voronoi tessellations (HVT). This step enables data insights, visualization, and interaction with the topology preserving map. Useful for semi-supervised tasks.

4.  **Scoring**: Scoring new data sets and recording their assignment using the map objects from the above steps, in a sequence of maps if required.

# 2. Example : HVT with the Torus dataset

## 2.1.1 Import Dataset from Local

The user can provide an absolute or relative path in the cell below to
access the data from his/her computer. User can set
`import_data_from_local` variable to TRUE to upload dataset from local.
<br> **Note: For this notebook import_data_from_local has been set to
FALSE as we are simulating a dataset in next section.**

```{r loading csv, message=FALSE, warning=FALSE}
import_data_from_local = FALSE # expects logical input

  file_name <- " " #enter the file name
  file_path <- " " #enter the path of the file in local.

# Loading the data in the Rstudio environment 
# Please change the path in the code line below to the path location of the .csv file
if(import_data_from_local){
  file_load <- paste0(file_path, file_name)
  dataset_updated <- as.data.frame(fread(file_load))
  if(nrow(dataset_updated) > 0){
    paste0("File ", file_name, " having ", nrow(dataset_updated), " row(s) and ", ncol(dataset_updated), " column(s)",  " imported successfully. ") %>% cat("\n")
    # Round only the numeric columns in dataset
    dataset_updated <- dataset_updated %>% mutate_if(is.numeric, round, digits = 4)
    paste0("Code chunk executed successfully. Below table showing first 10 row(s) of the dataset.") %>% cat("\n")
    # Display imported dataset
    dataset_updated %>% head(10) %>% 
      as.data.frame() %>%
      DT::datatable(options = options, rownames = TRUE)
  }
  
} 
```

# 2.1.2 Simulate Dataset

In this section, we will use a simulated dataset. Given below is a simulated
dataset called torus that contains 12000 observations and 3 features. 

Let us see how to generate data for torus. We are using a library `geozoo` for this purpose. 
Geo Zoo (stands for Geometric Zoo) is a compilation of geometric objects ranging from 
3 to 10 dimensions. Geo Zoo contains regular or well-known objects, eg cube and sphere, and some abstract objects, e.g. Boy's surface, Torus and Hyper-Torus.

Here, we load the data and store into a variable `dataset_updated`.

```{r}
set.seed(240)
##torus data generation
torus <- geozoo::torus(p = 3,n = 12000)
dataset_updated <- data.frame(torus$points)
colnames(dataset_updated) <- c("x","y","z")


  if(nrow(dataset_updated) > 0){
paste0( "Dataset having ", nrow(dataset_updated), " row(s) and ", ncol(dataset_updated), " column(s)",  "simulated successfully. ") %>% cat("\n")  
    # Round only the numeric columns in dataset
    dataset_updated <- dataset_updated %>% mutate_if(is.numeric, round, digits = 4) 
    paste0("Code chunk executed successfully. The table below is showing first 20 row(s) of the dataset.") %>% cat("\n")
    # Display imported dataset
    dataset_updated %>% head(100) %>% 
      as.data.frame() %>%
      Table(scroll = TRUE, limit = 20)
  }

```

------------------------------------------------------------------------

# 3. Data Understanding

## 3.1 Quick Peek of the Data

**Summary of Dataset**

The table below shows summary of all (numeric & categorical) columns of
Dataset

```{r sum_test,  message=FALSE, warning=FALSE}
calculate_statistics <- function(column_data) {
     if (is.numeric(column_data)) {
         a <- min(column_data, na.rm = TRUE)
         b <- as.numeric(quantile(column_data, probs = 0.25, na.rm = TRUE)[1])
         c <- median(column_data, na.rm = TRUE)
         d <- mean(column_data, na.rm = TRUE)
         e <- sd(column_data, na.rm = TRUE)
         f <- as.numeric(quantile(column_data, probs = 0.75, na.rm = TRUE)[1])
         g <- max(column_data, na.rm = TRUE)
         
         # Combine the statistics into a data frame and set row names to an empty string
         stats_data <- data.frame(Min = a, Q1 = b, Median = c, Mean = d, sd = e, Q3 = f, Max = g)
         row.names(stats_data) <- ""
     } else {
         cat("Column is not numeric and was skipped.\n")
         return(NULL)
     }
     return(stats_data)
 }

# Apply the function to each column of dataset_updated_train
statistics_list <- lapply(dataset_updated, calculate_statistics)

# Print the result
print(statistics_list)
```

**Structure of Data**

In the below section we can see the structure of the data.

```{r struc_test3,  warning=FALSE, message=FALSE, error=FALSE}
dataset_updated %>% str()
```

## 3.2 Deleting Irrelevant Columns

The cell below will allow user to drop irrelevant column.

```{r Deleting irrelevant columns, warning=FALSE, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# Add column names which you want to remove
want_to_delete_column <- "no"

    del_col<-c(" `column_name` ")  

if(want_to_delete_column == "yes"){
   dataset_updated <-  dataset_updated[ , !(names(dataset_updated) %in% del_col)]
  print("Code chunk executed successfully. Overview of data types after removed selected columns")
  str( dataset_updated)
}else{
  paste0("No Columns removed. Please enter column name if you want to remove that column") %>% cat("\n")
}


```

## 3.3 Formatting and Renaming Columns

The code below contains a user defined function to rename or reformat
any column that the user chooses.

```{r Formatting and renaming columns, warning=FALSE, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# convert the column names to lower case
colnames( dataset_updated) <- colnames( dataset_updated) %>% casefold()

## rename column ?
want_to_rename_column <- "no" ## type "yes" if you want to rename a column

## renaming a column of a dataset 
rename_col_name <- " 'column_name` " ## use small letters
rename_col_name_to <- " `new_name` "

if(want_to_rename_column == "yes"){
  names( dataset_updated)[names( dataset_updated) == rename_col_name] <- rename_col_name_to
}

# remove space, comma, dot from column names
spaceless <- function(x) {colnames(x) <- gsub(pattern = "[^[:alnum:]]+",
                             replacement = ".",
                             names(x));x}
 dataset_updated <- spaceless( dataset_updated)

## below is the dataset summary
paste0("Successfully converted the column names to lower case and check the renamed column name if you changed") %>% cat("\n")
str( dataset_updated) ## showing summary for updated 

```

## 3.4 Changing Data Type of Columns

The section allows the user to change the data type of columns of his/her
choice.

```{r column type, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# If you want to change column type, change a below variable value to "yes"
want_to_change_column_type <- "no"

# you can change column type into numeric or character only
change_column_to_type <- "character" ## numeric

if(want_to_change_column_type == "yes" && change_column_to_type == "character"){
########################################################################################
################################## User Input Needed ###################################
########################################################################################
  select_columns <- c("panel_var") ###### Add column names you want to change here #####
   dataset_updated[select_columns]<- sapply( dataset_updated[select_columns],as.character)
  paste0("Code chunk executed successfully. Datatype of selected column(s) have been changed into numerical.")
  #str( dataset_updated)
}else if(want_to_change_column_type == "yes" && change_column_to_type == "numeric"){
  select_columns <- c('gearbox_oil_temperature')
   dataset_updated[select_columns]<- sapply( dataset_updated[select_columns],as.numeric)
  paste0("Code chunk executed successfully. Datatype of selected column(s) have been changed into categorical.")
  #str( dataset_updated)
}else{
  paste0("Datatype of columns have not been changed.") %>% cat("\n")
}
dataset_updated <- do.call(data.frame, dataset_updated)
str( dataset_updated)

```

## 3.5 Checking and Removing Duplicates

Presence of duplicate observations can be misleading, this sections
helps get rid of such rows in the dataset.

```{r,  message=FALSE, warning=FALSE, purl = FALSE}

want_to_remove_duplicates <- "yes"  ## type "no" for choosing to not remove duplicates

## removing duplicate observation if present in the dataset
if(want_to_remove_duplicates == "yes"){
  
   dataset_updated <-  dataset_updated %>% unique()
  paste0("Code chunk executed successfully, duplicates if present successfully removed. Updated dataset has ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s)") %>% print()
  cat("\n")
  str( dataset_updated) ## showing summary for updated dataset
} else{
  paste0("Code chunk executed successfully, NO duplicates were removed") %>% print()
}
```

## 3.6 List of Numerical and Categorical Column Names

```{r, warning=FALSE, message=FALSE, purl = FALSE}

# Return the column type 
CheckColumnType <- function(dataVector) {
  #Check if the column type is "numeric" or "character" & decide type accordingly
  if (class(dataVector) == "integer" || class(dataVector) == "numeric") {
    columnType <- "numeric"
  } else { columnType <- "character" }
  #Return the result
  return(columnType)
}
### Loading the list of numeric columns in variable
numeric_cols <<- colnames( dataset_updated)[unlist(sapply( dataset_updated, 
                                                       FUN = function(x){ CheckColumnType(x) == "numeric"}))]

### Loading the list of categorical columns in variable
cat_cols <- colnames( dataset_updated)[unlist(sapply( dataset_updated, 
                                                   FUN = function(x){ 
                                                     CheckColumnType(x) == "character"|| CheckColumnType(x) == "factor"}))]

### Removing Date Column from the list of categorical column
paste0("Code chunk executed successfully, list of numeric and categorical variables created.") %>% cat()
paste0("\n\n Numerical Column(s): \n Count : ", length(numeric_cols), "\n") %>% cat()
paste0(numeric_cols) %>% print()
paste0("\n Categorical Column(s): \n Count : ", length(cat_cols), "\n") %>% cat()
paste0(cat_cols) %>% print()
```

## 3.7 Filtering Dataset for Analysis

In this section, the dataset can be filtered for required row(s) for
further analysis.

```{r,  message=FALSE, warning=FALSE, purl = FALSE}

want_to_filter_dataset <- "no" ## type "yes" in case you want to filter
filter_col <- " "  ## Enter Column name to filter
filter_val <- " "        ## Enter Value to exclude for the column selected

if(want_to_filter_dataset == "yes"){
   dataset_updated <- filter_at( dataset_updated
                              , vars(contains(filter_col))
                              , all_vars(. != filter_val))
  
  paste0("Code chunk executed successfully, dataset filtered successfully on required columns. Updated dataset has ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s)") %>% print()
  cat("\n")
  str( dataset_updated) ## showing summary for updated dataset
  
} else{
  paste0("Code chunk executed successfully, entire dataset is available for analysis.") %>% print()
}
```

------------------------------------------------------------------------

## 3.8 Missing Value Analysis

Missing values in the training data can lead to a biased model because
we have not analyzed the behavior and relationship of those values with
other variables correctly. It can lead to a wrong calculation or
classification. Missing values can be of 3 types:

-   **Missing Completely At Random (MCAR)**: When missing data are MCAR,
    the presence/absence of data is completely independent of observable
    variables and parameters of interest. This is a case when the
    probability of missing variables is the same for all observations.
    For example, respondents of the data collection process decide that
    they will declare they're earning after tossing a fair coin. If a
    head occurs, the respondent declares his / her earnings & vice
    versa.
-   **Missing At Random (MAR)**: When missing data is not random but can
    be related to an observed variable where there is complete
    information. This kind of missing data can induce a bias in the
    analysis, especially if it unbalances the data because of many
    missing values in a certain category. For example, we are collecting
    data for age and female has higher missing value compare to male.
-   **Missing Depending on Unobserved Predictors**: This is a case when
    the missing values are not random and are related to the unobserved
    input variable.

**Missing Value on Entire dataset**

```{r miss_value_entire_data, warning=FALSE, purl = FALSE}
na_total <- sum(is.na( dataset_updated))/prod(dim( dataset_updated))
if(na_total == 0){
  paste0("In the uploaded dataset, there is no missing value") %>% cat("\n")
}else{
  na_percentage <- paste0(sprintf(na_total*100, fmt = '%#.2f'),"%")
  paste0("Percentage of missing value in entire dataset is ",na_percentage) %>% cat("\n")
}

```

**Missing Value on Column-level**

The following code is to visualize the missing values (if any) using bar
chart.

`gg_miss_upset` function are using to visualize the patterns of
missingness, or rather the combinations of missingness across cases.

This function gives us(if any missing value present):

-   how many variables have missing values
-   which variable has the most missing values
-   And give us the variables which have missing values together

```{r miss_value_column_level, warning=FALSE,,fig.width=10, purl = FALSE}

# Below code gives you missing value in each column
paste0("Number of missing value in each column") %>% cat("\n")
print(sapply( dataset_updated, function(x) sum(is.na(x))))

missing_col_names <- names(which(sapply( dataset_updated, anyNA)))

total_na <- sum(is.na( dataset_updated))
# visualize the missing values (if any) using bar chart
if(total_na > 0 && length(missing_col_names) > 1){
  paste0("Code chunk executed successfully. Visualizing the missing values using bar chart") %>% cat("\n")
  gg_miss_upset( dataset_updated,
  nsets = 10,
  nintersects = NA)
}else if(total_na > 0){
   dataset_updated %>%
  DataExplorer::plot_missing() 
  # paste0("Code chunk executed successfully. Only one column ",missing_col_names," have missing values ", sum(is.na( dataset_updated)),".") %>% cat("\n")
}else{
  paste("Code chunk executed successfully. No missing value exist.") %>% cat("\n")
}

```

**Missing Value Treatment**

In this section user can make decisions, how to tackle missing values in
dataset. Both column(s) and row(s) can be removed in the following
dataset based on the user choose to do so.


**Drop Column(s) with Missing Values**

The below code accepts user input and deletes the specified column.

```{r Col_Missing_Value_treatment,, purl = FALSE}

########################################################################################
################################## User Input Needed ###################################
########################################################################################

# OR do you want to drop column specific column
drop_cloumn_name_na <- "yes" ## type "yes" to drop column(s)
# write column name that you want to drop
drop_column_name <- c(" ") #enter column name
if(drop_cloumn_name_na == "yes"){
  names_df=names( dataset_updated) %in% drop_column_name
  dataset_updated <-  dataset_updated[ , which(!names( dataset_updated) %in% drop_column_name)]
  paste0("Code chunk executed, selected column(s) dropped successfully.") %>% print()
  cat("\n")
  str( dataset_updated)
} else {
  paste0("Code chunk executed, missing value not removed (if any).") %>% cat("\n")
  cat("\n")
}
```

**Drop Row(s) with Missing Values**

The below code accepts user input and deletes rows.

```{r Row_Missing_Value_treatment, message=FALSE, warning=FALSE, , purl = FALSE}

# Do you want to drop row(s) containing "NA"
drop_row <- "no" ## type "yes" to delete missing value observations
if(drop_row == "yes"){
  
  # imputing blank with NAs and removing all rows containing NAs
  #  dataset_updated[ dataset_updated == ""] <- NA
  # removing missing values from data
   dataset_updated <-  dataset_updated %>% na.omit()
  
  paste0("Code chunk executed, missing values successfully identified and removed. Updated dataset has ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s)") %>% print()
  cat("\n")
  # str( dataset_updated)
  
} else{
  paste0("Code chunk executed, missing value(s) not removed (if any).") %>% cat("\n")
  cat("\n")
}
```

### 3.8.1 One-Hot Encoding

This technique bins all categorical values as either 1 or 0. It is used
for categorical variables with 2 classes. This is done because
classification models can only handle features that have numeric values.

Given below is the length of unique values in each categorical column

```{r}
cat_cols <-
  colnames(dataset_updated)[unlist(sapply(
    dataset_updated,
    FUN = function(x) {
      CheckColumnType(x) == "character" ||
        CheckColumnType(x) == "factor"
    }
  ))]

apply(dataset_updated[cat_cols], 2, function(x) {
  length(unique(x))
})
```

Selecting categorical columns with smaller unique values for
dummification

```{r dummify,purl = TRUE}
########################################################################################
################################## User Input Needed ###################################
########################################################################################
# Do you want to dummify the categorical variables?

dummify_cat <- FALSE ## TRUE,FALSE

# Select the columns on which dummification is to be performed
dum_cols <- c("location.type","class")
########################################################################################
```

```{r dummify1, purl=FALSE,echo=dummify_cat}

# Below we dummify the columns mentioned by the user in the above code chunk
if(dummify_cat){
dummified_cols <- dataset_updated %>% dplyr::select(dum_cols) %>%       
  dummies::dummy.data.frame(dummy.classes = "ALL", sep = "_")  # dummy encode columns
names(dummified_cols) <- gsub(pattern = "[^[:alnum:]]+",
                             replacement = ".",
                             names(dummified_cols))

dataset_updated <- dataset_updated %>% 
  cbind(dummified_cols) %>%     # append encoded columns
  dplyr::select(-dum_cols)      # remove the old categorical columns
print("Shown below are the One-Hot Encoded columns")
print(dummified_cols %>% head(5) %>% DT::datatable(options = options) ) # Display columns
dummified_cols=names(dummified_cols)

# here we manipulate column names, round numeric values and display first 100 rows of data
colnames(dataset_updated) <- gsub(pattern = "[^[:alnum:]]+",
                             replacement = ".",
                             names(dataset_updated)) # Removes white-spaces between column names

dataset_updated <- dataset_updated %>% dplyr::mutate_if(is.numeric,round,digits = 3) # rounding digits in numeric values

dataset_updated %>% head(100) %>% DT::datatable(options = options)
} else{
 print("One-Hot Encoding was not performed on dataset.") 
}

```

### 3.8.2 Check for Singularity {#singularity}

```{r,purl = FALSE}
# Check data for singularity
singular_cols <- sapply(dataset_updated,function(x) length(unique(x))) %>%  # convert to dataframe
  data.frame(Unique_n = .) %>% dplyr::filter(Unique_n == 1) %>% 
  rownames() %>% data.frame(Constant_Variables = .)

if(nrow(singular_cols) != 0) {                              
  singular_cols  %>% DT::datatable()
} else {
  paste("There are no singular columns in the dataset") %>% htmltools::HTML()
}
# Display variance of columns
data <- dataset_updated %>% dplyr::summarise_if(is.numeric, var) %>% t() %>% 
  data.frame() %>% round(3) #%>% DT::datatable(colnames = "Variance")

colnames(data) <- c("Variance")
Table(data,scroll = FALSE)

```

### 3.8.3 Selecting only Numeric Cols after Dummification

```{r}
numeric_cols=as.vector(sapply(dataset_updated, is.numeric))
dataset_updated=dataset_updated[,numeric_cols]
colnames(dataset_updated)
```



## 3.9 Final Dataset Summary

All further operations will be performed on the following dataset.

```{r Final_Dataset_Summary, ,warning=FALSE, purl = FALSE}
nums <- colnames(dataset_updated)[unlist(lapply(dataset_updated, is.numeric))]
cat(paste0("Final data frame contains ", nrow( dataset_updated), " row(s) and ", ncol( dataset_updated), " column(s).","Code chunk executed. Below table showing first 10 row(s) of the dataset."))
dataset_updated <-  dataset_updated %>% mutate_if(is.numeric, round, digits = 4)

dataset_updated %>% head(10) %>%
  as.data.frame() %>%
  Table(scroll = FALSE)

```

```{r,echo=TRUE,eval=FALSE}
      DT::datatable(
        dataset_updated %>%
          select_if(., is.numeric) %>%
          skimr::skim() %>%
          mutate_if(is.numeric, round, digits = 4) %>%
          rename_at(.vars = vars(starts_with("skim_")), .funs = funs(sub("skim_", "", .))) %>%
          rename_at(.vars = vars(starts_with("numeric.")), .funs = funs(sub("numeric.", "", .))) %>%
          select(-c(type, n_missing, complete_rate)) %>%
          mutate(n_row = nrow(dataset_updated),
                 n_missing = rowSums(is.na(.))
                 # ,n_non_missing = n_row - n_missing
                 ) ,
        selection = "none",
        # filter = "top",
        class = 'cell-border stripe',
        escape = FALSE,
        options = options,
        callback = htmlwidgets::JS(
          "var tips = ['Index showing column number',
                        'Columns used for building the HVT model',
                        'Histogram for individual column',
                        'Number of records for each feature',
                        'Number of missing (NA) values for each feature',
                        'Mean of individual column',
                        'Standard deviation of individual column',
                        '0th Percentile means that the values are smaller than all 100% of the rows',
                        '25th Percentile means that the values are bigger than 25% and smaller than only 75% of the rows',
                        '50th Percentile means that the values are bigger than 50% and smaller than only 50% of the rows',
                        '75th Percentile means that the values are bigger than 75% and smaller than only 25% of the rows',
                        '100th Percentile means that the values are bigger than 100% of the rows'],
                            header = table.columns().header();
                        for (var i = 0; i < tips.length; i++) {
                          $(header[i]).attr('title', tips[i]);
                        }"
        )
      )
```

```{r}
#print(
 aa <-  dataset_updated %>%
    select_if(., is.numeric) %>%
    skimr::skim() %>%
    mutate_if(is.numeric, round, digits = 4) %>%
    rename_at(.vars = vars(starts_with("skim_")), .funs = funs(sub("skim_", "", .))) %>%
    rename_at(.vars = vars(starts_with("numeric.")), .funs = funs(sub("numeric.", "", .))) %>%
    select(-c(type, n_missing, complete_rate)) %>%
    mutate(n_row = nrow(dataset_updated),
           n_missing = rowSums(is.na(.))
           # ,n_non_missing = n_row - n_missing
           )
Table(aa,scroll = TRUE, limit = 20)
```

# 4. Data distribution

**Variable Histograms**

Shown below is the distribution of all the variables in the dataset.

```{r hist_plot, purl=TRUE, echo = FALSE,include=FALSE}
# Here we define a function to generate histograms to plot distribution

generateDistributionPlot <- function(data, column){
  # browser()
  p1<- ggplot2::ggplot(data,
                       ggplot2::aes(x = data[[column]])) +
    ggplot2::geom_histogram(ggplot2::aes(y = ..count..), fill = "midnightblue", size = 0.2, alpha=0.7) +
    ggplot2::xlab(column) + ggplot2::ylab("Count") +
    ggplot2::labs(column) +ggplot2::theme_bw() + 
    ggplot2::theme(panel.border=ggplot2::element_rect(size=0.1),legend.position = c(0.8, 0.8))
  
  p2<-ggplot2::ggplot(data, ggplot2::aes(x = data[[column]])) +
    ggplot2::stat_function(ggplot2::aes(color = "Normal"), fun = dnorm,args = list(mean = mean(data[[column]]),sd = sd(data[[column]]))) +
    ggplot2::stat_density(ggplot2::aes(color = "Density"), geom = "line", linetype = "dashed")  +
    ggplot2::scale_colour_manual("", values = c("black", "#EE7600")) +
    ggplot2::scale_linetype_manual("", values = c("Normal" = 2, "Density" = 1))  +
    ggplot2::guides(
      fill = ggplot2::guide_legend(keywidth = 1, keyheight = 1),
      linetype = ggplot2::guide_legend(keywidth = 3, keyheight = 1),
      colour = ggplot2::guide_legend(keywidth = 3, keyheight = 1)) + 
    ggplot2::ylab("Density") +
    ggplot2::xlab("Density") +
    ggplot2::theme(
      panel.background = ggplot2::element_blank(),
      panel.grid.minor = ggplot2::element_blank(),
      panel.grid.major = ggplot2::element_blank(),
      axis.text.y = ggplot2::element_text(colour="black", size=8),
      axis.text.x = ggplot2::element_text(size = 14),
      axis.ticks = ggplot2::element_line(colour = 'gray50'),
      axis.ticks.x = ggplot2::element_line(colour = "black"))
  
  g1 <- ggplot2::ggplotGrob(p1)
  g2 <- ggplot2::ggplotGrob(p2)
  
  pp <- c(subset(g1$layout, name == "panel", se = t:r))
  
  # superimpose p2 (the panel) on p1
  g <- gtable::gtable_add_grob(g1, g2$grobs[[which(g2$layout$name == "panel")]], pp$t, 
                               pp$l, pp$b, pp$l)
  
  # extract the y-axis of p2
  ia <- which(g2$layout$name == "axis-l")
  ga <- g2$grobs[[ia]]
  ax <- ga$children[[2]]
  
  # flip it horizontally
  ax$widths <- rev(ax$widths)
  ax$grobs <- rev(ax$grobs)
  
  # add the flipped y-axis to the right
  g <- gtable::gtable_add_cols(g, g2$widths[g2$layout[ia, ]$l], length(g$widths) - 1)
  g <- gtable::gtable_add_grob(g, ax, pp$t, length(g$widths) - 1, pp$b)
  distHistogram <- g
  return(distHistogram)
}

```

```{r dist_plots_true2, purl = FALSE,message=FALSE,warning=FALSE, fig.height=6, fig.width=6}
eda_cols <- names(dataset_updated)
# Here we plot the distribution of columns selected by user for numerical transformation
dist_list <- lapply(1:length(eda_cols), function(i){
generateDistributionPlot(dataset_updated, eda_cols[i]) })
do.call(gridExtra::grid.arrange, args = list(grobs = dist_list, ncol = 2, top = "Distribution of Features"))
 
```

**Box Plots**

In this section, we plot box plots for each numeric column in the
dataset across panels. These plots will display the median and Inter
Quartile Range of each column at a panel level.

```{r, results='asis', warning = FALSE, purl = FALSE}
## the below function helps plotting quantile outlier plot for multiple variables
quantile_outlier_plots_fn <- function(data, outlier_check_var, data_cat = dataset_updated[, cat_cols], numeric_cols = numeric_cols){
    # lower threshold
    lower_threshold <- stats::quantile(data[, outlier_check_var], .25,na.rm = TRUE) - 1.5*(stats::IQR(data[, outlier_check_var], na.rm = TRUE))
    
    # upper threshold
    upper_threshold <- stats::quantile(data[,outlier_check_var],.75,na.rm = TRUE) + 1.5*(stats::IQR(data[,outlier_check_var],na.rm = TRUE))
    
    # Look for outliers based on thresholds
    data$QuantileOutlier <- data[,outlier_check_var] > upper_threshold | data[,outlier_check_var] < lower_threshold

  # Plot box plot
  quantile_outlier_plot <- ggplot2::ggplot(data, ggplot2::aes(x="", y = data[,outlier_check_var])) +
             ggplot2::geom_boxplot(fill = 'blue',alpha=0.7) + 
             ggplot2::theme_bw() + 
             ggplot2::theme(panel.border=ggplot2::element_rect(size=0.1),panel.grid.minor.x=ggplot2::element_blank(),panel.grid.major.x=ggplot2::element_blank(),legend.position = "bottom") + ggplot2::ylab(outlier_check_var) + ggplot2::xlab("")
  data <- cbind(data[, !names(data) %in% c("QuantileOutlier")] %>% round(2), outlier = data[, c("QuantileOutlier")])
  data <- cbind(data, data_cat)  
  return(list(quantile_outlier_plot, data, lower_threshold, upper_threshold))
}
```

```{r message=FALSE, warning=FALSE,results='asis', fig.height=6, fig.width=6}
## the below code gives the interactive plot for Quantile Outlier analysis for numerical variables 
box_plots <- list()
for (x in names(dataset_updated)) {

box_plots[[x]] <- quantile_outlier_plots_fn(data = dataset_updated, outlier_check_var = x)[[1]]

}

gridExtra::grid.arrange(grobs = box_plots, ncol = 3)

```

**Correlation Matrix**

In this section we are calculating **pearson correlation** which is a
bivariate correlation value measuring the linear correlation between two
numeric columns. The output shown is a matrix.

```{r Correlation, echo=FALSE, fig.height=6, fig.width=6, purl=FALSE}

corrplot::corrplot(cor(dataset_updated), 
                            method = "color", 
                            outline = TRUE, 
                            addgrid.col = "darkgray",
                            # order="hclust", 
                            addrect = 4,
                            rect.col = "black",
                            rect.lwd = 5,
                            cl.pos = "b",
                            tl.col = "black", 
                            tl.cex = 1, 
                            cl.cex = 1,
                            addCoef.col = "black",
                            number.digits = 2, 
                            number.cex = 1.5, 
                            type = "lower",
                            col = grDevices::colorRampPalette(c("#982D3B","white","#235C89"))(200))
  
# For better visualisation use the 'show in the new window' in the right corner of the output code block and in full screen if you have more columns

```

# 4.1 Train - Test Split

Let us first split the data into train and test. We will use 80% of the
data as train and remaining as test.

```{r train-test dataset_updated,warning=FALSE,message=FALSE,eval = global_var}
## 80% of the sample size
smp_size <- floor(0.80 * nrow(dataset_updated))

## set the seed to make your partition reproducible
set.seed(240)
train_ind <- sample(seq_len(nrow(dataset_updated)), size = smp_size)

dataset_updated_train <- dataset_updated[train_ind, ]
dataset_updated_test <- dataset_updated[-train_ind, ]
```

The train data contains `r nrow(dataset_updated_train)` rows and
`r ncol(dataset_updated_train)` columns. The test data contains
`r nrow(dataset_updated_test)` rows and `r ncol(dataset_updated_test)`
columns.

### 4.1.1 Train Distribution

```{r dist_plots_true23, purl = FALSE,message=FALSE,warning=FALSE, fig.height=6, fig.width=6}
eda_cols <- names(dataset_updated_train)
# Here we plot the distribution of columns selected by user for numerical transformation
dist_list <- lapply(1:length(eda_cols), function(i){
generateDistributionPlot(dataset_updated_train, eda_cols[i]) })
do.call(gridExtra::grid.arrange, args = list(grobs = dist_list, ncol = 2, top = "Distribution of Features"))
 
```

### 4.1.2 Test Distribution

```{r dist_plots_true234, purl = FALSE,message=FALSE,warning=FALSE, fig.height=6, fig.width=6}
eda_cols <- names(dataset_updated_test)
# Here we plot the distribution of columns selected by user for numerical transformation
dist_list <- lapply(1:length(eda_cols), function(i){
generateDistributionPlot(dataset_updated_test, eda_cols[i]) })
do.call(gridExtra::grid.arrange, args = list(grobs = dist_list, ncol = 2, top = "Distribution of Features"))
 
```


# 5. Map A : Base Compressed Map

Let us try to visualize the compressed Map A from the flow diagram below.

```{r ,echo=FALSE,warning=FALSE,fig.show='hold',message=FALSE,out.width='90%',fig.height=8,fig.cap='Figure 1: Data Segregation with highlighted bounding box in red around compressed map A'}
knitr::include_graphics('scoreLayeredHVT_function_mapA.png')
```

This package can perform vector quantization using the following algorithms -

-   Hierarchical Vector Quantization using k−means
-   Hierarchical Vector Quantization using k−medoids

For more information on vector quantization, refer the following [link](https://htmlpreview.github.io/?https://github.com/Mu-Sigma/HVT/blob/master/vignettes/HVT_vignette.html#compress-vector-quantization).

The trainHVT function constructs highly compressed hierarchical Voronoi tessellations. The raw data is first scaled and this scaled data is supplied as input to the vector quantization algorithm. The vector quantization algorithm compresses the dataset until a user-defined compression percentage/rate is achieved using a parameter called quantization error which acts as a threshold and determines the compression percentage. It means that for a given user-defined compression percentage we get the 'n' number of cells, then all of these cells formed will have a quantization error less than the threshold quantization error.

Let's try to comprehend the **trainHVT function** first before moving ahead.

```{r trainHVT function, echo=TRUE, eval=FALSE}
trainHVT(
  dataset,
  min_compression_perc,
  n_cells,
  depth,
  quant.err,
  distance_metric = c("L1_Norm", "L2_Norm"),
  error_metric = c("mean", "max"),
  quant_method = c("kmeans", "kmedoids"),
  normalize = TRUE,
  diagnose = FALSE,
  hvt_validation = FALSE,
  train_validation_split_ratio = 0.8
)
```

Each of the parameters of trainHVT function have been explained below:

-   **`dataset`** - A dataframe, with numeric columns (features) that will be used for training the model.

-   **`min_compression_perc`** - An integer, indicating the minimum compression percentage to be achieved for the dataset. It indicates the desired level of reduction in dataset size compared to its original size.

-   **`n_cells`** - An integer, indicating the number of cells per hierarchy (level). This parameter determines the granularity or level of detail in the hierarchical vector quantization.

-   **`depth`** - An integer, indicating the number of levels. A depth of 1 means no hierarchy (single level), while higher values indicate multiple levels (hierarchy).

-   **`quant.error`** - A number indicating the quantization error threshold. A cell will only breakdown into further cells if the quantization error of the cell is above the defined quantization error threshold.

-   **`projection.scale`** - A number indicating the scale factor for the tesselations so as to visualize the sub-tesselations well enough. It helps in adjusting the visual representation of the hierarchy to make the sub-tesselations more visible.

-   **`scale_summary`** - A list with mean and standard deviation values for all the features in the dataset. Pass the scale summary when the input dataset is already scaled or normalize is set to False.

-   **`distance_metric`** - The distance metric can be `L1_Norm`(Manhattan) or `L2_Norm`(Eucledian). `L1_Norm` is selected by default. The distance metric is used to calculate the distance between an `n` dimensional point and centroid.

-   **`error_metric`** - The error metric can be `mean` or `max`. `max` is selected by default. `max` will return the max of `m` values and `mean` will take mean of `m` values where each value is a distance between a point and centroid of the cell.

-   **`quant_method`** - The quantization method can be `kmeans` or `kmedoids`. Kmeans uses means (centroids) as cluster centers while Kmedoids uses actual data points (medoids) as cluster centers. `kmeans` is selected by default.

-   **`normalize`** - A logical value indicating if the dataset should be normalized. When set to TRUE, scales the values of all features to have a mean of 0 and a standard deviation of 1 (Z-score)

-   **`diagnose`** - A logical value indicating whether user wants to perform diagnostics on the model. The 4th element of the hvt.result (list) contains detailed informations for debugging. Default value is TRUE.

-   **`hvt_validation`** - A logical value indicating whether user wants to holdout a validation set and find mean absolute deviation of the validation points from the centroid. These values can be found in the 6th element of the hvt.result (list) under the model.info under the title validation_result. Default value is FALSE.

-   **`train_validation_split_ratio`** - A numeric value indicating train validation split ratio. This argument is only used when hvt_validation has been set to TRUE. Default value for the argument is 0.8.

The output of trainHVT function (list of 6 elements) have been explained below:

-   The '1st element' is a list containing information related to plotting tessellations. This information might include coordinates, boundaries, or other details necessary for visualizing the tessellations

-   The '2nd element' is a list containing information related to Sammon's projection coordinates of the data points in the reduced-dimensional space.

-   The '3rd element' is a list containing detailed information about the hierarchical vector quantized data along with a summary section containing no of points, Quantization Error and the centroids for each cell.

-   The '4th element' is a list that contains all the diagnostics information of the model when diagnose is set to TRUE.Otherwise NA

-   The '5th element' is a list that contains all the information required to generates a Mean Absolute Deviation (MAD) plot, if hvt_validation is set to TRUE. Otherwise NA

-   The '6th element' (model info) is a list that contains model generated timestamp, input parameters passed to the model and the validation results.

We will use the `trainHVT` function to compress our data while preserving essential features of the dataset. Our goal is to achieve data compression upto atleast `80%`. In situations where the compression ratio does not meet the desired target, we can explore adjusting the model parameters as a potential solution. This involves making modifications to parameters such as the `quantization error threshold` or `increasing the number of cells` and then rerunning the trainHVT function again.

As this is already done in  [**HVT Vignette:**](https://htmlpreview.github.io/?https://github.com/Mu-Sigma/HVT/blob/master/vignettes/HVT_vignette.html)  please refer for more information.

**Model Parameters**

* Number of Cells at each Level = 900
* Maximum Depth = 1
* Quantization Error Threshold = 0.1
* Error Metric = Max
* Distance Metric = Manhattan

```{r torus hvt third ,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 5: The Voronoi tessellation for layer 1 shown for the 900 cells in the dataset ’torus’',eval = global_var}
set.seed(240)
torus_mapA <- trainHVT(
  dataset_updated_train,
  n_cells = 900,
  depth = 1,
  quant.err = 0.1,
  projection.scale = 10,
  normalize = FALSE,
  distance_metric = "L1_Norm",
  error_metric = "max",
  quant_method = "kmeans"
)

```


Let's check the compression summary for torus.

```{r compression summary torus third,warning=FALSE,eval = global_var}
compressionSummaryTable(torus_mapA[[3]]$compression_summary)
```

We successfully compressed 85% of the data using n_cells parameter as 900, the next step involves performing data projection on the compressed data. In this step, the compressed data will be transformed and projected onto a lower-dimensional space to visualize and analyze the data in a more manageable form.

As per the manual, **`torus_mapA[[3]]`** gives us detailed information about the hierarchical vector quantized data. **`torus_mapA[[3]][['summary']]`** gives a nice tabular data containing no of points, Quantization Error and the codebook.

The datatable displayed below is the **summary from torus_mapA** showing Cell.ID, Centroids and Quantization Error for each of the 900 cells.

```{r hvt_mapA display results, warning=FALSE, echo=TRUE}
summaryTable(torus_mapA[[3]]$summary,scroll = TRUE,limit = 500)

```

Now let us understand what each column in the above table means:

-   **`Segment.Level`** - Level of the cell. In this case, we have performed Vector Quantization for depth 1. Hence Segment Level is 1.

-   **`Segment.Parent`** - Parent segment of the cell.

-   **`Segment.Child (Cell.Number)`** - The children of a particular cell. In this case, it is the total number of cells at which we achieved the defined compression percentage.

-   **`n`** - No of points in each cell.

-   **`Cell.ID`** - Cell_ID's are generated for the multivariate data using 1-D Sammon's Projection algorithm.

-   **`Quant.Error`** - Quantization Error for each cell.

All the columns after this will contain centroids for each cell. They can also be called a codebook, which represents a collection of all centroids or codewords.


Now let's try to understand **plotHVT** function. The parameters have been explained in detail below:

```{r plotHVT function,echo = TRUE, eval= FALSE}
plotHVT <-(hvt.results, line.width, color.vec, pch1 = 21, palette.color = 6, centroid.size = 1.5, title = NULL, maxDepth = NULL, dataset, child.level, hmap.cols, previous_level_heatmap = TRUE, show.points = FALSE, asp = 1, ask = TRUE, tess.label = NULL, quant.error.hmap = NULL, n_cells.hmap = NULL, label.size = 0.5, sepration_width = 7, layer_opacity = c(0.5, 0.75, 0.99), dim_size = 1000, heatmap = '2Dhvt') 
```

*  **`hvt.results`** - (1D/2Dhvt/2Dheatmap/surface_plot)  A list (hvt.result) obtained from the trainHVT function while performing hierarchical vector quantization on training data. This list provides an overview of the hierarchical vector quantized data, including diagnostics, tessellation details, Sammon's projection coordinates, and model input information.

* **`line.width`**	- (2Dhvt/2Dheatmap) A vector indicating the line widths of the tessellation boundaries for each layer.

* **`color.vec`**	- (2Dhvt/2Dheatmap) A vector indicating the colors of the tessellations boundaries at each layer.

* **`pch1`**	- (2Dhvt/2Dheatmap) Symbol, It plots the centroids with a particular symbol such as (solid circle, bullet, filled square, filled diamond) in the tessellations.(default = 21 i.e filled circle).

* **`centroid.size`**	- (2Dhvt/2Dheatmap) Size of centroids for each level of tessellations (default = 3).

* **`title`**	-  (2Dhvt) Set a title for the plot (default = NULL).

* **`maxDepth`** -  (2Dhvt) An integer indicating the number of levels. (default = NULL)

* **`palette.color`** - (2Dheatmap) A number indicating the heat map color palette. 1 - rainbow,2 - heat.colors, 3 - terrain.colors, 4 - topo.colors, 5 - cm.colors, 6 - BlCyGrYlRd (Blue,Cyan,Green,Yellow,Red) color (default= 6).

* **`dataset`** - (2Dheatmap) Data frame. The input dataset.

* **`child.level`** - (2Dheatmap/surface_plot) A Number indicating the level for which the heat map is to be plotted.

* **`hmap.cols`** - (2Dheatmap/surface_plot) A Number or a Character which is the column number of column name from
the dataset indicating the variables for which the heat map is to be plotted.

* **`previous_level_heatmap`** -  (2Dheatmap) A Logical value to indicate whether the heatmap of previous level
will be overlayed on the heatmap of selected level.

* **`show.points`** - (2Dheatmap) A Logical value indicating if the centroids should be plotted on the tessellations.
 (default = FALSE)
 
* **`asp`** - (2Dheatmap) A Number indicating the aspect ratio type. For flexible aspect
ratio set, asp = NA. (default = 1)

* **`ask`** - (2Dheatmap) A Logical value to have interactive R session.(default = TRUE) 

* **`tess.label`** - (2Dheatmap) A vector for labelling the tessellations. (default
 = NULL)

* **`label.size`** - (2Dheatmap) The size by which the tessellation labels should
be scaled.(default = 0.5)

* **`quant.error.hmap`** - (2Dheatmap) A number indicating the quantization error threshold.

* **`n_cells.hmap`** - (2Dheatmap) An integer indicating the number of cells/clusters per hierarchy 

* **`sepration_width`** - (surface_plot) An integer indicating the width between two Levels.

* **`layer_opacity`** - (surface_plot) A vector indicating the opacity of each layer/ level.

* **`dim_size`** - (surface_plot) An integer indicating the dimension size used to create the matrix for the plot.

* **`heatmap`** -  A Character indicating which type of plot should be generated. Accepted entries are '1D','2Dhvt','2Dheatmap', 'surface_plot'. Default value is 2Dhvt.



Let's plot the Voronoi tessellation for layer 1 (map A).

```{r, warning=FALSE,message=FALSE,fig.height=4, fig.width=6,fig.cap='Figure 2: The Voronoi Tessellation for layer 1 (map A) shown for the 900 cells in the dataset ’torus’'}
plotHVT(torus_mapA,
        line.width = c(0.4), 
        color.vec = c("#141B41"),
        centroid.size = 0.01,
        maxDepth = 1, heatmap = '2Dhvt') 
```

**Heat Maps**

We will now overlay all the features as heatmap over the Voronoi Tessellation plot for better visualization and identification of patterns, trends, and variations in the data.

Now let's plot the Voronoi Tessellation with the heatmap overlaid for all the features in the torus data for better visualization and interpretation of data patterns and distributions.

The heatmaps displayed below provides a visual representation of the spatial characteristics of the torus data, allowing us to observe patterns and trends in the distribution of each of the features (x,y,z). The sheer green shades highlight regions with higher values in each of the heatmaps, while the indigo shades indicate areas with the lowest values in each of the heatmaps. By analyzing these heatmaps, we can gain insights into the variations and relationships between each of these features within the torus data.


```{r hvt_mapA hmp level x torus,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 4: The Voronoi Tessellation with the heat map overlaid for variable ’x’ in the ’torus’ dataset',eval = global_var,fig.height=4, fig.width=6}

  plotHVT(
  torus_mapA,
  dataset_updated_train,
  child.level = 1,
  hmap.cols = "x",
  line.width = c(0.2),
  color.vec = c("#141B41"),
  palette.color = 6,
  centroid.size = 0.1,
  show.points = TRUE,
  quant.error.hmap = 0.2,
  n_cells.hmap = 900,
  heatmap = '2Dheatmap'
) 

```

```{r hvt_mapA hmp level y torus,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 5: The Voronoi Tessellation with the heat map overlaid for variable ’y’ in the ’torus’ dataset',eval = global_var,fig.height=4, fig.width=6}

  plotHVT(
  torus_mapA,
  dataset_updated_train,
  child.level = 1,
  hmap.cols = "y",
  line.width = c(0.2),
  color.vec = c("#141B41"),
  palette.color = 6,
  centroid.size = 0.1,
  show.points = TRUE,
  quant.error.hmap = 0.2,
  n_cells.hmap = 900,
  heatmap = '2Dheatmap'
) 

```

```{r hvt_mapA hmp  level one z torus,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 6: The Voronoi Tessellation with the heat map overlaid for variable ’z’ in the ’torus’ dataset',eval = global_var,fig.height=4, fig.width=6}

plotHVT(
  torus_mapA,
  dataset_updated_train,
  child.level = 1,
  hmap.cols = "z",
  line.width = c(0.2),
  color.vec = c("#141B41"),
  palette.color = 6,
  centroid.size = 0.1,
  show.points = TRUE,
  quant.error.hmap = 0.2,
  n_cells.hmap = 440,
  heatmap = '2Dheatmap'
) 

```



# 5. Map B : Compressed Novelty Map

Let us try to visualize the Map B from the flow diagram below.

```{r ,echo=FALSE,warning=FALSE,fig.show='hold',message=FALSE,out.width='90%',fig.height=8,fig.cap='Figure 10: Data Segregation with highlighted bounding box in red around map B'}
knitr::include_graphics('scoreLayeredHVT_function_mapB.png')
```

In this section, we will manually figure out the novelty cells from the plotted torus_mapA and store it in **identified_Novelty_cells** variable.

**Note**: For manual selecting the novelty cells from map A, one can enhance its interactivity by adding plotly elements to the code. This will transform map A into an interactive plot, allowing users to actively engage with the data. By hovering over the centroids of the cells, a tag containing segment `child` information will be displayed. Users can explore the map by hovering over different cells and selectively choose the novelty cells they wish to consider. Added an image for reference.

```{r ,echo=FALSE,warning=FALSE,fig.show='hold',message=FALSE,out.width='90%',fig.height=8,fig.cap='Figure 11: Manually selecting novelty cells'}
knitr::include_graphics('reference.png')
```

The **`removeNovelty`** function removes the identified novelty cell(s) from the training dataset (containing 9600 datapoints) and stores those records separately.

It takes input as the cell number (Segment.Child) of the manually identified novelty cell(s) and the compressed HVT map (torus_mapA) with 900 cells. It returns a list of two items: `data with novelty`, and a subset of the `data without novelty`.

*NOTE:* As we are using `torus data` here, the identified novelty cells given are for demo purpose.

```{r remove_novelty,results='asis', message=FALSE}
identified_Novelty_cells <<- c(347,504,855,887,138,649,522,853)   #as a example
output_list <- removeNovelty(identified_Novelty_cells, torus_mapA)
data_with_novelty <- output_list[[1]]
data_without_novelty <- output_list[[2]]
```

Let's have a look at the **data with novelty**(containing 104 records). For the sake of brevity, we will only show the first 20 rows.

```{r hvt_mapB results, warning=FALSE, echo=TRUE}
novelty_data <- data_with_novelty
novelty_data$Row.No <- row.names(novelty_data)
novelty_data <- novelty_data %>% dplyr::select("Row.No","Cell.ID","Cell.Number","x","y","z")
colnames(novelty_data) <- c("Row.No","Cell.ID","Segment.Child","x","y","z")
novelty_data %>% head(100) %>% 
  as.data.frame() %>%
  Table(scroll = TRUE, limit = 20)
```

```{=html}
<style>
/* Add space after the table */
table {
  margin-bottom: 20px;
}
</style>
```
## 5.1 Voronoi Tessellation with highlighted novelty cell

The **`plotNovelCells`** function is used to plot the Voronoi tessellation using the compressed HVT map (torus_mapA) containing 900 cells and highlights the identified novelty cell(s) i.e 8 cells (containing 104 records) in red on the map.

```{r compress_novelty_plot,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 12: The Voronoi Tessellation constructed using the compressed HVT map (map A) with the novelty cell(s) highlighted in red',eval = global_var,fig.height=4, fig.width=6}
plotNovelCells(identified_Novelty_cells, torus_mapA,line.width = c(0.4),centroid.size = 0.01)
```

We pass the dataframe with novelty records (104 records) to trainHVT function along with other model parameters mentioned below to generate **map B** (layer2)

**Model Parameters**

-   Number of Cells at each Level = 17
-   Maximum Depth = 1
-   Quantization Error Threshold = 0.1
-   Error Metric = Max
-   Distance Metric = Manhattan

```{r, message=FALSE,warning=FALSE}
colnames(data_with_novelty) <- c("Cell.ID","Segment.Child","x","y","z")
data_with_novelty <- data_with_novelty[,-1:-2]
torus_mapB <- list()
mapA_scale_summary = torus_mapA[[3]]$scale_summary
torus_mapB <- trainHVT(data_with_novelty,
                  n_cells = 17,   
                  depth = 1,
                  quant.err = 0.1,
                  projection.scale = 10,
                  normalize = FALSE,
                  distance_metric = "L1_Norm",
                  error_metric = "max",
                  quant_method = "kmeans"
                  )

```

The datatable displayed below is the **summary from map B (layer 2)** showing Cell.ID, Centroids and Quantization Error for each of the 17 cells.

```{r, message=FALSE,warning=FALSE}
summaryTable(torus_mapB[[3]]$summary,scroll = TRUE)

```

Now let's check the compression summary for HVT (torus_mapB). The table below shows no of cells, no of cells having quantization error below threshold and percentage of cells having quantization error below threshold for each level.

```{r hvt_map_B compression summary,warning=FALSE,eval = global_var}
mapB_compression_summary <- torus_mapB[[3]]$compression_summary %>%  dplyr::mutate_if(is.numeric, funs(round(.,4)))
compressionSummaryTable(mapB_compression_summary)
```

**As it can be seen from the table above, `88%`** of the cells have hit the quantization threshold error.Since we are successfully able to attain the desired compression percentage, so we will not further subdivide the cells

# 6. Map C : Compressed Map without Novelty

Let us try to visualize the compressed Map C from the flow diagram below.

```{r ,echo=FALSE,warning=FALSE,fig.show='hold',message=FALSE,out.width='90%',fig.height=8,fig.cap='Figure 13:Data Segregation with highlighted bounding box in red around compressed map C'}
knitr::include_graphics('scoreLayeredHVT_function_mapC.png')
```

### 6.1 [Iteration 1:]{style="color:blue"}

With the Novelties removed, we construct another hierarchical Voronoi tessellation **map C** layer 2 on the data without Novelty (containing 9496 records) and below mentioned model parameters.

**Model Parameters**

-   Number of Cells at each Level = 10
-   Maximum Depth = 2
-   Quantization Error Threshold = 0.1
-   Error Metric = Max
-   Distance Metric = Manhattan

```{r,warning=FALSE,message=FALSE}
torus_mapC <- list()
mapA_scale_summary = torus_mapA[[3]]$scale_summary
torus_mapC <- trainHVT(data_without_novelty,
                  n_cells = 10,
                  depth = 2,
                  quant.err = 0.1,
                  projection.scale = 10,
                  normalize = FALSE,
                  distance_metric = "L1_Norm",
                  error_metric = "max",
                  quant_method = "kmeans",
                  diagnose = FALSE,
                  scale_summary = mapA_scale_summary)

```

Now let's check the compression summary for HVT (torus_mapC) where n_cell was set to 10. The table below shows no of cells, no of cells having quantization error below threshold and percentage of cells having quantization error below threshold for each level.

```{r hvt_map_C compression summary,warning=FALSE,eval = global_var}
mapC_compression_summary <- torus_mapC[[3]]$compression_summary %>%  dplyr::mutate_if(is.numeric, funs(round(.,4)))
compressionSummaryTable(mapC_compression_summary)
```

**As it can be seen from the table above, `0%`** of the cells have hit the quantization threshold error in level 1 and **`0%`** of the cells have hit the quantization threshold error in level 2

### 6.2 [Iteration 2:]{style="color:blue"}

Since, we are yet to achive atleast 80% compression at depth 2. Let's try to compress again using the below mentioned set of model parameters and the data without novelty (containing 9496 records).

**Model Parameters**

-   Number of Cells at each Level = 30
-   Maximum Depth = 2
-   Quantization Error Threshold = 0.1
-   Error Metric = Max
-   Distance Metric = Manhattan

```{r,warning=FALSE,message=FALSE}
torus_mapC <- list()
torus_mapC <- trainHVT(data_without_novelty,
                  n_cells = 30,    
                  depth = 2,
                  quant.err = 0.1,
                  projection.scale = 10,
                  normalize = FALSE,
                  distance_metric = "L1_Norm",
                  error_metric = "max",
                  quant_method = "kmeans",
                  diagnose = FALSE,
                  scale_summary = mapA_scale_summary)

```

The datatable displayed below is the **summary from map C (layer2)**. showing Cell.ID, Centroids and Quantization Error for each of the 925 cells.

```{r,message=FALSE,warning=FALSE}
summaryTable(torus_mapC[[3]]$summary,scroll = TRUE,limit = 500)

```

Now let's check the **compression summary** for HVT (torus_mapC). The table below shows no of cells, no of cells having quantization error below threshold and percentage of cells having quantization error below threshold for each level.

```{r hvt_mapC compression summary,warning=FALSE,eval = global_var}
mapC_compression_summary <- torus_mapC[[3]]$compression_summary %>%  dplyr::mutate_if(is.numeric, funs(round(.,4)))
compressionSummaryTable(mapC_compression_summary)
```

**As it can be seen from the table above, `85%`** of the cells have hit the quantization threshold error in level 1 and **`85%`** of the cells have hit the quantization threshold error in level 2

Let's plot the Voronoi tessellation for layer 2 (map C)

```{r, warning=FALSE,message=FALSE,fig.cap='Figure 14: The Voronoi Tessellation for layer 2 (map C) shown for the 924 cells in the dataset ’torus’ at level 2',fig.height=4, fig.width=6}
plotHVT(torus_mapC,
        line.width = c(0.4,0.2), 
        color.vec = c("#141B41","#0582CA"),
        centroid.size = 0.1,
        maxDepth = 2, 
        heatmap = '2Dhvt') 
```

**Heat Maps**

Now let's plot all the features for each cell at level two as a heatmap for better visualization.

The heatmaps displayed below provides a visual representation of the spatial characteristics of the torus data, allowing us to observe patterns and trends in the distribution of each of the features (x,y,z). The sheer green shades highlight regions with higher values in each of the heatmaps, while the indigo shades indicate areas with the lowest values in each of the heatmaps. By analyzing these heatmaps, we can gain insights into the variations and relationships between each of these features within the torus data.

```{r hvt_mapC hmp level one x torus,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 15: The Voronoi Tessellation with the heat map overlaid for feature `x` in the ’torus’ dataset' , eval = global_var,fig.height=4, fig.width=6}

  plotHVT(
  torus_mapC,
  dataset_updated_train,
  child.level = 2,
  hmap.cols = "x",
  line.width = c(0.6,0.4),
  color.vec = c("#141B41","#0582CA"),
  palette.color = 6,
  centroid.size = 0.1,
  show.points = TRUE,
  quant.error.hmap = 0.2,
  n_cells.hmap = 100,
  heatmap = '2Dheatmap'
) 

```

```{r hvt_mapC hmp level one y torus,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 16: The Voronoi Tessellation with the heat map overlaid for feature `y` in the ’torus’ dataset',eval = global_var,fig.height=4, fig.width=6}

  plotHVT(
  torus_mapC,
  dataset_updated_train,
  child.level = 2,
  hmap.cols = "y",
  line.width = c(0.6,0.4),
  color.vec = c("#141B41","#0582CA"),
  palette.color = 6,
  centroid.size = 0.1,
  show.points = TRUE,
  quant.error.hmap = 0.2,
  n_cells.hmap = 100,
  heatmap = '2Dheatmap'
) 

```

```{r hvt_mapC hmp level one z torus,warning=FALSE,fig.show='hold',results='hide',message=FALSE,fig.cap='Figure 17: The Voronoi Tessellation with the heat map overlaid for feature `z` in the ’torus’ dataset',eval = global_var,fig.height=4, fig.width=6}

  plotHVT(
  torus_mapC,
  dataset_updated_train,
  child.level = 2,
  hmap.cols = "z",
  line.width = c(0.6,0.4),
  color.vec = c("#141B41","#0582CA"),
  palette.color = 6,
  centroid.size = 0.1,
  show.points = TRUE,
  quant.error.hmap = 0.2,
  n_cells.hmap = 100,
  heatmap = '2Dheatmap'
) 

```


**We now have the set of maps (map A, map B & map C) which will be used to score, which map and cell each test record is assigned to, but before that lets view our test dataset**

# 7. Scoring on Test Data

Now once we have built the model, let us try to score using our test dataset (containing 2400 data points) which cell and which layer each point belongs to.

**Testing Dataset**

The testing dataset includes the following columns:

* x: This column represents the X-coordinate of each point in the torus.
* y: This column represents the Y-coordinate of each point in the torus.
* z: This column represents the Z-coordinate of each point in the torus.

Let's have a look at our randomly selected **test dataset** containing 2400 datapoints.

```{r sample test data torus,warning=FALSE,message=FALSE,eval = TRUE}
Table(head(dataset_updated_test))
```

The **scoreLayeredHVT** function is used to score the test data using the scored set of maps. This function takes an input - a test data and a set of maps (map A, map B, map C).

Now, Let us understand the **`scoreLayeredHVT`** function.

```{r scoreLayeredHVT function,echo = TRUE, eval= FALSE}
scoreLayeredHVT(data,
                map_A,
                map_B,
                map_C,
                mad.threshold = 0.2,
                normalize = TRUE, 
                distance_metric="L1_Norm",
                error_metric="max",
                child.level = 1, 
                line.width = c(0.6, 0.4, 0.2),
                color.vec = c("#141B41", "#6369D1", "#D8D2E1"),
                yVar= NULL,
                ...)
```

Each of the parameters of **scoreLayeredHVT** function has been explained below:

-   **`data`** - A dataframe containing the test dataset. The dataframe should have all the variable(features) used for training.

-   **`map A`** - Result (hvt.result) obtained from HVT function while performing hierarchical vector quantization on train data. This list containes information about the hierarchical vector quantized data along with a summary section.

-   **`map B`** - Result (hvt.result) obtained from HVT function while performing hierarchical vector quantization on novelty data. Novelty data is a subset of the training data obtained as a result of removeNovelty function (1st element).

-   **`map C`** - Result (hvt.result) obtained from HVT function while performing hierarchical vector quantization on training data without novelty. This data is a subset of the training data obtained as a result of removeNovelty function (2nd element).

-   **`child.level`** - A number indicating the depth for which the heat map is to be plotted (Only used if hmap.cols is not NULL), Each depth represents a different level of clustering or partitioning of the data.

-   **`mad.threshold`** - A threshold value used to filter data based on the Median Absolute Deviation (MAD) of the Quant.Error variable. It determines how extreme a deviation from the median has to be in order to consider as novelty

-   **`normalize`** - A logical value indicating if the dataset should be normalized. When set to TRUE, scales the values of all features to have a mean of 0 and a standard deviation of 1 (Z-score)

-   **`distance_metric`** - The distance metric can be `L1_Norm`(Manhattan) or `L2_Norm`(Eucledian). `L1_Norm` is selected by default. The distance metric is used to calculate the distance between an `n` dimensional point and centroid. The distance metric can be different from the one used during training.

-   **`error_metric`** - The error metric can be `mean` or `max`. `max` is selected by default. `max` will return the max of `m` values and `mean` will take mean of `m` values where each value is a distance between a point and centroid of the cell. The error metric can be different from the one used during training.

-   **`yVar`** - A character or a vector representing the name of the dependent variable(s)

-   **`line.width`** - A vector indicating the line widths of the tessellation boundaries for each layer. (Optional Parameters)

-   **`color.vec`** - A vector indicating the colors of the tessellations boundaries at each layer. (Optional Parameters)

The function score based on the HVT maps - map A, map B and map C, constructed using trainHVT function. For each test record, the function will assign that record to Layer1 or Layer2. Layer1 contains the cell ids from map A and Layer 2 contains cell ids from map B (novelty map) and map C (map without novelty).

**Scoring Algorithm**

The Scoring algorithm recursively calculates the distance between each point in the test dataset and the cell centroids for each level. The following steps explain the scoring method for a single point in the test dataset:

1.  Calculate the distance between the point and the centroid of all the cells in the first level.
2.  Find the cell whose centroid has minimum distance to the point.
3.  Check if the cell drills down further to form more cells.
4.  If it doesn't, return the path. Or else repeat steps 1 to 4 till we reach a level at which the cell doesn't drill down further.

**Note : The Scoring algorithm will not work if some of the variables used to perform quantization are missing. In the test dataset, we should not remove any features**

```{r, message=FALSE,warning=FALSE}

validation_data <- dataset_updated_test
new_score <- scoreLayeredHVT(
    data=validation_data,
    torus_mapA,
    torus_mapB,
    torus_mapC,
    normalize = FALSE
  )


```

Let's see which cell and layer each point belongs to and check the Mean Absolute Difference for each of the **2400 records**.

```{r}

act_pred <- new_score[["actual_predictedTable"]]
rownames(act_pred) <- NULL
act_pred %>% head(1000) %>%as.data.frame() %>%Table(scroll = TRUE)


```

```{r,message=FALSE,warning=FALSE,fig.cap='Figure 22: Mean Absolute Difference'}

hist(act_pred$diff, breaks = 30, col = "blue", main = "Mean Absolute Difference", xlab = "Difference")

```

# 8. Executive Summary

-   We have considered torus dataset for creating a scored sequence of maps using scoreLayeredHVT() in this vignette.

-   Our goal is to achieve data compression upto atleast `80%`.

-   We construct a compressed HVT map (torus_mapA) using the trainHVT() on the training dataset by setting **`n_cells`** to 900 and **`quant.error`** to 0.1 and we were able to attain a compression of 84%.

-   Based on the output of the above step, we manually identify the novelty cell(s) from the plotted map A. For this dataset, we identify 8 cells as the novelty cell. (since torus dataset does not have outliers we are using this for demo purpose.)

-   We pass the identified novelty cell(s) as a parameter to the removeNovelty() along with HVT torus_mapA. The function removes that novelty cell(s) from the dataset and stores them separately. It also returns the data without novelty(s).

-   The plotNovelCells() constructs hierarchical voronoi tessellations and highlights the identified novelty cell(s) in red.

-   The data with novelty is then passed to the trainHVT() to construct another HVT map (torus_mapB). But here, we set the parameters **`n_cells`** = 17, **`depth`** = 1 etc. when constructing the map.

-   The data without novelty is then passed to the trainHVT() to construct another HVT map (torus_mapC). But here, we set the parameters **`n_cells`** = 30, **`depth`** = 2 etc. when constructing the map.

-   Finally, the set of maps - torus_mapA,torus_mapB,torus_mapC are passed to the scoreLayeredHVT() along with the test dataset to score which map and what cell each test record is assigned to.

-   The output of scoreLayeredHVT is a dataset with two columns Layer1.Cell.ID and Layer2.Cell.ID. Layer1.Cell.ID contains cell ids from map A in the form A1,A2,A3.... and Layer2.Cell.ID contains cell ids from map B as B1,B2... depending on the identified novelties and map C as C1,C2,C3.....



# 9. References

1.  Topology Preserving Maps : <https://users.ics.aalto.fi/jhollmen/dippa/node9.html>

2.  Vector Quantization : <https://en.wikipedia.org/wiki/Vector_quantization>

3.  K-means : <https://en.wikipedia.org/wiki/K-means_clustering>

4.  Sammon's Projection : <https://en.wikipedia.org/wiki/Sammon_mapping>

5.  Voronoi Tessellations : <https://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation>
